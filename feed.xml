<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://0xgeorgeassaad.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://0xgeorgeassaad.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-11-14T21:00:53+00:00</updated><id>https://0xgeorgeassaad.github.io/feed.xml</id><title type="html">blank</title><subtitle>I learn by tinkering, and tinker to keep learning. </subtitle><entry><title type="html">Everything You Need to Know About Backpropagation</title><link href="https://0xgeorgeassaad.github.io/blog/2025/backprop/" rel="alternate" type="text/html" title="Everything You Need to Know About Backpropagation"/><published>2025-10-18T00:00:00+00:00</published><updated>2025-10-18T00:00:00+00:00</updated><id>https://0xgeorgeassaad.github.io/blog/2025/backprop</id><content type="html" xml:base="https://0xgeorgeassaad.github.io/blog/2025/backprop/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>No algorithm is more fundamental to deep learning than backpropagation. While the concept itself is beautifully simple, peeking under the hood of modern frameworks like PyTorch or TensorFlow reveals a labyrinth of device dispatchers, type handlers, and optimization layers that obscure the elegant mathematics beneath. Until JAX came along, which exposes a very elegant API for exploring and defining backprop operations very easily.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/cards/backprop-card-480.webp 480w,/assets/img/cards/backprop-card-800.webp 800w,/assets/img/cards/backprop-card-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/cards/backprop-card.png" class="img-fluid rounded d-block mx-auto" width="670px" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">The word 'Backprop' written using the same style as JAX logo</figcaption> </figure> <p>You might wonder: if modern frameworks handle differentiation automatically, why bother understanding backpropagation at all? The answer becomes clear the moment you need to push beyond standard operations. When optimizing critical bottlenecks with custom kernels in CUDA, Pallas (JAX), or Triton, you’re suddenly responsible for defining how gradients flow through your code. Without understanding backpropagation’s core mechanics (how to compute and propagate gradients correctly), you’ll struggle to write kernels that integrate seamlessly into your training loop. Even a foundational grasp of VJPs and the chain rule transforms your understanding of how deep learning models are trained.</p> <h2 id="the-core-theory-of-backpropagation">The Core Theory of Backpropagation</h2> <p>At its heart, a neural network is a series of nested mathematical functions. We start with an input, pass it through the first function (or “layer”) to get an intermediate result, pass that result through the next layer, and so on, until we get a final output. To “train” the network, we need to systematically adjust the parameters (the weights and biases) of each function to minimize the final error. Backpropagation is the clever and efficient algorithm that tells us exactly how to do this. It works by calculating the gradient, or the rate of change of the final error with respect to each parameter in the network, allowing an optimization algorithm like gradient descent to update the parameters in the right direction.</p> <p>To make this concrete, let’s trace the flow of data through a simple two-layer neural network, which will be our working example for the rest of this post.</p> \[\begin{align*} z_1 &amp;= f_1(x) = W_1 x \hspace{2cm} &amp;\text{// } x: (d_1, 1),\; W_1: (d_2, d_1),\; z_1: (d_2, 1) \\ a &amp;= \tanh(z_1) &amp;\text{// } a: (d_2, 1) \\z_2 &amp;= f_2(a) = W_2 a &amp;\text{// } W_2: (d_3, d_2),\; z_2: (d_3, 1) \\ y &amp;= \text{Softmax}(z_2) &amp;\text{// } y: (d_3, 1) \\ \mathcal{L} &amp;= \text{Loss}(y) &amp;\text{// } \mathcal{L}: \text{scalar} \end{align*}\] <h3 id="the-chain-rule-with-vector-jacobian-products-vjp">The Chain Rule with Vector-Jacobian Products (VJP)</h3> <p>This is the single most essential mathematical concept for understanding backpropagation:</p> <blockquote> <p><strong>Definition 1 (Vector-Jacobian Product)</strong> &gt; <br/> <br/> Let $y = f(x)$, $x \in \mathbb{R}^n, \quad y \in \mathbb{R}^m, \quad \mathcal{L} = \mathcal{L}(y)$ is a scalar loss function; then</p> \[\nabla_{x} \mathcal{L} = J_f(x)^T \nabla_{y} \mathcal{L}\] <p class="block-note">where $J_f(x) = \frac{\partial f(x)}{\partial x} \in \mathbb{R}^{m \times n}$.</p> </blockquote> <p>The key insight of backpropagation is understanding what the vector $\nabla_{y} \mathcal{L}$ represents. It is the “upstream gradient”, the gradient of the final scalar loss with respect to the output $y$ of the function $f$. The VJP $J_f(x)^T \nabla_{y} \mathcal{L}$ then gives us the “local gradient”, the gradient of the final scalar loss with respect to the function’s input $x$. Backpropagation is, in essence, a chain of these VJP calculations, passing the upstream gradient from the last layer all the way back to the first.</p> <p>Now, we can get the equation for computing the gradient of the loss with the weights of our furthest layer as such:</p> \[\begin{align*}\nabla_{z_2} \mathcal{L} &amp;= J_{y}^T \nabla_{y} \mathcal{L} \\\nabla_{a} \mathcal{L} &amp;= J_{z_2}^T \nabla_{z_2} \mathcal{L} = J_{z_2}^T J_{y}^T \nabla_{y} \mathcal{L} \\\nabla_{z_1} \mathcal{L} &amp;= J_{a}^T \nabla_{a} \mathcal{L}= J_{a}^T J_{z_2}^T J_{y}^T \nabla_{y} \mathcal{L} \\ \nabla_{W_1} \mathcal{L} &amp;= J_{z_1}^T \nabla_{z_1} \mathcal{L} = J_{z_1}^T J_{a}^T J_{z_2}^T J_{y}^T \nabla_{y} \mathcal{L} \end{align*}\] <p>It’s very important to take a moment and realize there are two ways to compute the gradient, one could start computing from left to right in a fashion known as <em>“Forward mode automatic differentiation”</em> or start computing from right to left in a fashion known as <em>“Backward mode automatic differentiation”</em>. The latter is the one used in training neural networks and commonly referred to as “backpropagation” or “backprop” for short.</p> \[\underbrace{\nabla_{W_1} \mathcal{L}}_{(d_2 \times d_1,\, d_2)} = \underbrace{J_{z_1}^T}_{(d_2 \times d_1,\, d_2)} \ \underbrace{J_{a}^T}_{(d_2,\, d_2)} \ \underbrace{J_{z_2}^T}_{(d_2,\, d_3)} \ \underbrace{J_{y}^T}_{(d_3,\, d_3)} \ \underbrace{\nabla_{y} \mathcal{L}}_{(d_3,\, 1)}\] <h3 id="forward-vs-reverse-mode-why-backprop-wins">Forward vs. Reverse Mode: Why Backprop Wins</h3> <p>To avoid any hand-waving, I won’t move on until I show you fully why backprop is a vastly more efficient method in training deep learning models compared to forward mode autodiff. (If you read this and aren’t yet convinced, leave a comment and I will try to provide another example).</p> <p>Even though in reality we need to compute $\nabla_{W_1} \mathcal{L}$, I will dissect the computation of $\nabla_{z_1} \mathcal{L}$ mainly because the leftmost jacobian turns out to be a simple outer product (we will show later that we never materialize any of the jacobians shown here).</p> \[\underbrace{\nabla_{z_1} \mathcal{L}}_{(d_2, 1)} = \underbrace{J_{a}^T}_{(d_2,\, d_2)} \ \underbrace{J_{z_2}^T}_{(d_2,\, d_3)} \ \underbrace{J_{y}^T}_{(d_3,\, d_3)} \ \underbrace{\nabla_{y} \mathcal{L}}_{(d_3,\, 1)}\] <p>In forward mode autodiff, we start from the left and compute all matrix multiplications as such:</p> \[\nabla_{z_1} \mathcal{L} = ((J_{a}^T J_{z_2}^T )J_{y}^T) \nabla_{y} \mathcal{L}\] <p>which yields the number of multiplications as follows (remember multiplying $(m,n) \times (n,p)$ matrices requires $mnp$ multiplications)</p> <p><strong>Total No. of Multiplications for Forward Mode</strong>: $d_2^2 d_3 + d_2 d_3^2 + d_2 d_3$</p> <p>In backward mode autodiff, we start from the right and compute all matrix multiplications as such:</p> \[\nabla_{z_1} \mathcal{L} = J_{a}^T (J_{z_2}^T (J_{y}^T \nabla_{y} \mathcal{L}))\] <p>which yields the number of multiplications as follows</p> <p><strong>Total No. of Multiplications for Reverse Mode</strong>: $d_3^2 + d_2 d_3 + d_2^2$</p> <p>The reason backpropagation is universally used for training neural networks becomes clear when we compare the total costs, especially with dimensions typical for these models. In most neural networks, the hidden layers are much wider than the output layer ($d_2 \gg d_3$).</p> <p>Let’s consider a simple classification problem with a 500-neuron hidden layer ($d_2 = 500$) and 10 output classes ($d_3 = 10$).</p> <ul> <li> <p><strong>Reverse Mode Cost</strong> = $10^2 + (500 \times 10) + 500^2 = 100 + 5,000 + 250,000 = \boldsymbol{255,100}$ operations.</p> </li> <li> <p><strong>Forward Mode Cost</strong> = $(500^2 \times 10) + (500 \times 10^2) + (500 \times 10) = 2,500,000 + 50,000 + 5,000 = \boldsymbol{2,555,000}$ operations.</p> </li> </ul> <p>In this realistic scenario, <strong>forward mode is 10 times more computationally expensive than reverse mode</strong>.</p> <p>This dramatic difference stems from the fact that forward mode requires performing computationally heavy matrix-matrix multiplications, creating large intermediate matrices. Reverse mode (backpropagation) cleverly avoids this by only ever performing matrix-vector multiplications, which is significantly cheaper.</p> <h2 id="the-vjp-trick-never-materialize-jacobians">The VJP Trick: Never Materialize Jacobians</h2> <p>While we’ve established that backprop is the efficient approach for computing gradients, our algorithm can be made even faster and more memory-efficient by avoiding an expensive computational step: materializing Jacobian matrices.</p> <p>We will show a very powerful trick, we <em>never</em> need to materialize any jacobian to pass on the gradients to the inputs.</p> <h3 id="deriving-the-gradient-for-a-linear-layer">Deriving the Gradient for a Linear Layer</h3> <p>Let’s demonstrate this principle using the most fundamental operation in neural networks: matrix multiplication. Consider a linear transformation defined as:</p> \[\begin{align*} y &amp;= Wx \\ \nabla_{W} \mathcal{L} &amp;= J_y^T \nabla_{y} \mathcal{L} \quad \text{From Definition 1 above} \end{align*}\] <p>Let’s expand the above equation fully to observe that the jacobian is <em>sparse</em> (mostly zeros) and simplify the operation further.</p> <p>Let’s define the dimensions: $x \in \mathbb{R}^{d_1 \times 1}$, $W \in \mathbb{R}^{d_2 \times d_1}$, and $y \in \mathbb{R}^{d_2 \times 1}$. The components of the output are $y_i = \sum_{j=1}^{d_1} W_{ij} x_j$.</p> <p>The Jacobian of $y$ with respect to the flattened weight matrix is a $(d_2, d_2 d_1)$ matrix. The derivative $\frac{\partial y_k}{\partial W_{ij}}$ is non-zero only when $k=i$, resulting in a very sparse Jacobian:</p> \[J_y = \begin{pmatrix} \underbrace{x_1 \ \dots \ x_{d_1}}_{W_{1j}} &amp; \underbrace{0 \ \dots \ 0}_{W_{2j}} &amp; \cdots &amp; \underbrace{0 \ \dots \ 0}_{W_{d_2,j}} \\ 0 \ \dots \ 0 &amp; x_1 \ \dots \ x_{d_1} &amp; \cdots &amp; 0 \ \dots \ 0 \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ 0 \ \dots \ 0 &amp; 0 \ \dots \ 0 &amp; \cdots &amp; x_1 \ \dots \ x_{d_1} \end{pmatrix}\] <p>The transposed Jacobian $J_y^T$ is a $(d_2 d_1, d_2)$ matrix. We multiply this by the gradient vector $\nabla_y \mathcal{L} \in \mathbb{R}^{d_2 \times 1}$:</p> \[\nabla_W \mathcal{L} = J_y^T \nabla_y \mathcal{L} = \begin{pmatrix} x_1 &amp; 0 &amp; \cdots &amp; 0 \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ x_{d_1} &amp; 0 &amp; \cdots &amp; 0 \\[0.4em] \hline 0 &amp; x_1 &amp; \cdots &amp; 0 \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ 0 &amp; x_{d_1} &amp; \cdots &amp; 0 \\[0.4em] \hline \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\[0.4em] \hline 0 &amp; 0 &amp; \cdots &amp; x_1 \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ 0 &amp; 0 &amp; \cdots &amp; x_{d_1} \end{pmatrix} \begin{pmatrix} \frac{\partial \mathcal{L}}{\partial y_1} \\ \frac{\partial \mathcal{L}}{\partial y_2} \\ \vdots \\ \frac{\partial \mathcal{L}}{\partial y_{d_2}} \end{pmatrix} = \begin{pmatrix} x_1 \frac{\partial \mathcal{L}}{\partial y_1} \\ \vdots \\ x_{d_1} \frac{\partial \mathcal{L}}{\partial y_1} \\[0.4em] \hline x_1 \frac{\partial \mathcal{L}}{\partial y_2} \\ \vdots \\ x_{d_1} \frac{\partial \mathcal{L}}{\partial y_2} \\[0.4em] \hline \vdots \\[0.4em] \hline x_1 \frac{\partial \mathcal{L}}{\partial y_{d_2}} \\ \vdots \\ x_{d_1} \frac{\partial \mathcal{L}}{\partial y_{d_2}} \end{pmatrix}\] <p>Reshaping this flattened $(d_2 d_1, 1)$ vector back to the $(d_2, d_1)$ dimensions of $W$, we get:</p> \[\nabla_W \mathcal{L} = \begin{pmatrix} \frac{\partial \mathcal{L}}{\partial y_1}x_1 &amp; \frac{\partial \mathcal{L}}{\partial y_1}x_2 &amp; \dots &amp; \frac{\partial \mathcal{L}}{\partial y_1}x_{d_1} \\ \frac{\partial \mathcal{L}}{\partial y_2}x_1 &amp; \frac{\partial \mathcal{L}}{\partial y_2}x_2 &amp; \dots &amp; \frac{\partial \mathcal{L}}{\partial y_2}x_{d_1} \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ \frac{\partial \mathcal{L}}{\partial y_{d_2}}x_1 &amp; \frac{\partial \mathcal{L}}{\partial y_{d_2}}x_2 &amp; \dots &amp; \frac{\partial \mathcal{L}}{\partial y_{d_2}}x_{d_1} \end{pmatrix}\] <p>This matrix is exactly the outer product of the column vector $\nabla_y \mathcal{L}$ and the row vector $x^T$:</p> \[\nabla_y \mathcal{L} \cdot x^T = \begin{pmatrix} \frac{\partial \mathcal{L}}{\partial y_1} \\ \frac{\partial \mathcal{L}}{\partial y_2} \\ \vdots \\ \frac{\partial \mathcal{L}}{\partial y_{d_2}} \end{pmatrix} \begin{pmatrix} x_1 &amp; x_2 &amp; \dots &amp; x_{d_1} \end{pmatrix} = \begin{pmatrix} \frac{\partial \mathcal{L}}{\partial y_1}x_1 &amp; \frac{\partial \mathcal{L}}{\partial y_1}x_2 &amp; \dots &amp; \frac{\partial \mathcal{L}}{\partial y_1}x_{d_1} \\ \frac{\partial \mathcal{L}}{\partial y_2}x_1 &amp; \frac{\partial \mathcal{L}}{\partial y_2}x_2 &amp; \dots &amp; \frac{\partial \mathcal{L}}{\partial y_2}x_{d_1} \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ \frac{\partial \mathcal{L}}{\partial y_{d_2}}x_1 &amp; \frac{\partial \mathcal{L}}{\partial y_{d_2}}x_2 &amp; \dots &amp; \frac{\partial \mathcal{L}}{\partial y_{d_2}}x_{d_1} \end{pmatrix}\] <p>Thus, the Vector-jacobian product (VJP) simplifies to the efficient outer product</p> \[\nabla_W \mathcal{L} = \boxed{ \nabla_y \mathcal{L} \cdot x^T }\] <p>I want you to take a moment to see why this is so efficient. $x$ is the input to our local operation which we already have and $\nabla_{y} \mathcal{L}$ is the gradient of the loss with respect to the output of our local operation and it’s being passed down through the computational graph. We successfully computed the final gradient without ever needing to materialize the full jacobian. This idea can be extended to any operation as long as you define how to pass the gradients from the outputs to the inputs which is exactly what we are going to show in code next using JAX.</p> <h3 id="implementing-custom-vjps-in-jax">Implementing Custom VJPs in JAX</h3> <p>JAX already has a built-in autodiff library that can compute the gradients of any function using <code class="language-plaintext highlighter-rouge">jax.numpy</code> primitives, but we are going to override that using a mechanism in JAX that allows you to write custom gradients for any function you write and then we are going to construct all operations needed to train the simple two layer classifier we have been working with all this time to train on MNIST dataset.</p> <p>Let’s start by defining the simplest operation of matrix multiplication, we will call it <code class="language-plaintext highlighter-rouge">Linear</code></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">jax</span>
<span class="kn">import</span> <span class="n">jax.numpy</span> <span class="k">as</span> <span class="n">jnp</span>

<span class="nd">@jax.custom_vjp</span>
<span class="k">def</span> <span class="nf">Linear</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">W</span> <span class="o">@</span> <span class="n">x</span>

<span class="k">def</span> <span class="nf">Linear_fwd</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="nc">Linear</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">x</span><span class="p">),</span> <span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">Linear_bwd</span><span class="p">(</span><span class="n">res</span><span class="p">,</span> <span class="n">g</span><span class="p">):</span>
    <span class="n">W</span><span class="p">,</span> <span class="n">x</span> <span class="o">=</span> <span class="n">res</span>
    <span class="n">grad_W</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">outer</span><span class="p">(</span><span class="n">g</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
    <span class="n">grad_x</span> <span class="o">=</span> <span class="n">W</span><span class="p">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">g</span>
    <span class="nf">return </span><span class="p">(</span><span class="n">grad_W</span><span class="p">,</span> <span class="n">grad_x</span><span class="p">)</span>

<span class="n">Linear</span><span class="p">.</span><span class="nf">defvjp</span><span class="p">(</span><span class="n">Linear_fwd</span><span class="p">,</span> <span class="n">Linear_bwd</span><span class="p">)</span>
</code></pre></div></div> <p>The <code class="language-plaintext highlighter-rouge">g</code> parameter in <code class="language-plaintext highlighter-rouge">Linear_bwd</code> represents the upstream gradient, that is, the gradient of the final loss with respect to the output of the Linear function.</p> <p>There are two things we need to understand here: first is that we need to define two functions and not just one for the backward pass <code class="language-plaintext highlighter-rouge">bwd</code>, the reason is we need to save some variables that we need during the backward pass, they are saved in a variable called residuals (<code class="language-plaintext highlighter-rouge">res</code>), second is that we need to compute the gradients for both $W$ and $x$ because even though we will only update $W$, we need the gradient to keep flowing through $x$ to the rest of the computational graph.</p> <p>Now let’s define the rest of the operations we need.</p> <p>To further solidify the VJP (Vector-Jacobian Product) concept, let’s consider an element-wise activation function like $a = \tanh(z)$. The derivative of $\tanh(z)$ is $1 - \tanh^2(z)$, and since the operation is applied element-wise, the Jacobian $J_a$ is a <strong>diagonal matrix</strong> with each diagonal entry being the derivative of the corresponding component:</p> \[J_a = \begin{pmatrix} 1 - a_1^2 &amp; 0 &amp; \cdots &amp; 0 \\ 0 &amp; 1 - a_2^2 &amp; \cdots &amp; 0 \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ 0 &amp; 0 &amp; \cdots &amp; 1 - a_{d_2}^2 \end{pmatrix}\] <p>Here, $a_i = \tanh(z_i)$, so each entry is $1 - \tanh^2(z_i) = 1 - a_i^2$.</p> <p>Since the Jacobian is diagonal and the operation is element-wise, the VJP (Vector-Jacobian Product) simplifies to a Hadamard (element-wise) product:</p> \[\nabla_z \mathcal{L} = J_a^T \nabla_a \mathcal{L} = J_a \nabla_a \mathcal{L} = (1 - a^2) \odot \nabla_a \mathcal{L}\] <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 2. Tanh Activation
</span><span class="nd">@jax.custom_vjp</span>
<span class="k">def</span> <span class="nf">Tanh</span><span class="p">(</span><span class="n">z</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">tanh</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">Tanh_fwd</span><span class="p">(</span><span class="n">z</span><span class="p">):</span>
    <span class="n">a</span> <span class="o">=</span> <span class="nc">Tanh</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">a</span><span class="p">,</span> <span class="p">(</span><span class="n">a</span><span class="p">,)</span>

<span class="k">def</span> <span class="nf">Tanh_bwd</span><span class="p">(</span><span class="n">res</span><span class="p">,</span> <span class="n">g</span><span class="p">):</span>
    <span class="n">a</span><span class="p">,</span> <span class="o">=</span> <span class="n">res</span>
    <span class="n">grad_z</span> <span class="o">=</span> <span class="n">g</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">a</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
    <span class="nf">return </span><span class="p">(</span><span class="n">grad_z</span><span class="p">,)</span> <span class="c1"># Gradient for the single input z
</span>
<span class="n">Tanh</span><span class="p">.</span><span class="nf">defvjp</span><span class="p">(</span><span class="n">Tanh_fwd</span><span class="p">,</span> <span class="n">Tanh_bwd</span><span class="p">)</span>
</code></pre></div></div> <h3 id="deriving-the-softmax-cross-entropy-gradient">Deriving the Softmax Cross-Entropy Gradient</h3> <p>The tanh is quite straightforward and you could easily verify it by hand. One important thing to realize is there is no restriction on the complexity of the operation as long as we <em>know</em> how to flow the gradients from the outputs to the inputs. So we have two options now either first define a softmax operation and then define another operation for the cross-entropy loss or combine them in a single operation. There are multiple reasons why you would want to combine operations into a single operation (e.g., numerical stability) but we will do because the gradients will be very simple. Let’s derive the gradients for the combined Softmax + Cross entropy operation and then implement <code class="language-plaintext highlighter-rouge">softmax_cross_entropy</code> function.</p> <p><strong>Note:</strong> here $y$ and $z$ refer to the logits and true labels respectively, not to be confused with the $y$ and $z$ used in the two-layer network example above.</p> <p>First, let’s define the core functions. We have a set of logits (the raw output of the last linear layer), $z = [z_1, z_2, …, z_C]$, where $C$ is the number of classes. The true label is a one-hot encoded vector $y = [y_1, y_2, …, y_C]$, where only one $y_k=1$ and all others are 0.</p> <p><strong>Softmax Function:</strong> Converts logits into probabilities. The probability for the $j$-th class, $p_j$, is:</p> \[p_j = \text{softmax}(z)_j = \frac{e^{z_j}}{\sum_{k=1}^{C} e^{z_k}}\] <p><strong>Cross-Entropy Loss:</strong> Calculates the loss based on the predicted probabilities $p$ and the true labels $y$.</p> \[\mathcal{L} = - \sum_{k=1}^{C} y_k \log(p_k)\] <p>The loss $\mathcal{L}$ is not a direct function of $z_i$. Instead, $\mathcal{L}$ depends on all the probabilities $p_1, p_2, …, p_C$, and each of these probabilities in turn depends on the logit $z_i$. Therefore, we must use the chain rule and sum over all paths through which $z_i$ affects $\mathcal{L}$:</p> \[\frac{\partial \mathcal{L} }{\partial z_i} = \sum_{j=1}^{C} \frac{\partial \mathcal{L} }{\partial p_j} \frac{\partial p_j}{\partial z_i}\] <p>We will solve this by calculating the two partial derivatives separately.</p> <p><strong>Derivative of Loss with respect to Probabilities</strong></p> <p>This part is straightforward. We differentiate the loss function $\mathcal{L} = - \sum_{k=1}^{C} y_k \log(p_k)$ with respect to a single probability $p_j$. Only the term where $k=j$ is non-zero.</p> \[\frac{\partial \mathcal{L} }{\partial p_j} = \frac{\partial}{\partial p_j} \left( - y_j \log(p_j) \right) = - \frac{y_j}{p_j}\] <p><strong>Derivative of Softmax with respect to Logits</strong></p> <p>This part is more complex because the output of softmax at index $j$ depends on all the input logits. We have two cases for $\frac{\partial p_j}{\partial z_i}$.</p> <p><strong>Case A: When $i = j$</strong> (e.g., differentiating $p_i$ with respect to $z_i$) We use the quotient rule on $p_i = \frac{e^{z_i}}{\sum_{k} e^{z_k}}$:</p> \[\begin{align*} \frac{\partial p_i}{\partial z_i} &amp;= \frac{(e^{z_i})' (\sum_{k} e^{z_k}) - (e^{z_i}) (\sum_{k} e^{z_k})'}{(\sum_{k} e^{z_k})^2}\\ &amp;= \frac{e^{z_i} (\sum_{k} e^{z_k}) - e^{z_i} (e^{z_i})}{(\sum_{k} e^{z_k})^2}\\ &amp;= \frac{e^{z_i}}{\sum_{k} e^{z_k}} - \left(\frac{e^{z_i}}{\sum_{k} e^{z_k}}\right)^2 = p_i - p_i^2 = p_i(1 - p_i) \end{align*}\] <p><strong>Case B: When $i \neq j$</strong> (e.g., differentiating $p_j$ with respect to $z_i$) Again, using the quotient rule on $p_j = \frac{e^{z_j}}{\sum_{k} e^{z_k}}$. This time, the numerator $e^{z_j}$ is a constant with respect to $z_i$.</p> \[\begin{align*} \frac{\partial p_j}{\partial z_i} &amp;= \frac{(e^{z_j})' (\sum_{k} e^{z_k}) - (e^{z_j}) (\sum_{k} e^{z_k})'}{(\sum_{k} e^{z_k})^2}\\ &amp;= \frac{0 \cdot (\sum_{k} e^{z_k}) - e^{z_j} (e^{z_i})}{(\sum_{k} e^{z_k})^2}\\ &amp;= - \left(\frac{e^{z_j}}{\sum_{k} e^{z_k}}\right) \left(\frac{e^{z_i}}{\sum_{k} e^{z_k}}\right) = - p_j p_i \end{align*}\] <p>Now we substitute these results back into our chain rule sum. We split the sum into two parts: the term where $j=i$ and all the terms where $j \neq i$.</p> \[\frac{\partial \mathcal{L} }{\partial z_i} = \underbrace{\frac{\partial \mathcal{L} }{\partial p_i} \frac{\partial p_i}{\partial z_i}}_{\text{Term for } j=i} + \underbrace{\sum_{j \neq i} \frac{\partial \mathcal{L} }{\partial p_j} \frac{\partial p_j}{\partial z_i}}_{\text{Terms for } j \neq i}\] <p>Substitute our findings from the previous steps and simplify</p> \[\begin{align*} \frac{\partial \mathcal{L} }{\partial z_i} &amp;= \left(-\frac{y_i}{p_i}\right) \cdot \left(p_i(1-p_i)\right) + \sum_{j \neq i} \left(-\frac{y_j}{p_j}\right) \cdot \left(-p_j p_i\right)\\ &amp;= -y_i(1-p_i) + \sum_{j \neq i} y_j p_i \\ &amp;= -y_i + y_i p_i + p_i \sum_{j \neq i} y_j \\ &amp;= -y_i + p_i \left( y_i + \sum_{j \neq i} y_j \right) \\ &amp;= -y_i + p_i(1) \\ &amp;= \boxed{p_i - y_i} \end{align*}\] <p>This remarkable result shows that the gradient of the combined softmax and cross-entropy loss with respect to a single logit $z_i$ is simply the difference between the predicted probability for that class and the true label for that class.</p> <p>When we generalize this from a single logit to the entire vector of logits $z$, the gradient vector is:</p> \[\nabla_z \mathcal{L} = p - y\] <p>This is precisely what we implemented in the following <code class="language-plaintext highlighter-rouge">softmax_cross_entropy_bwd</code> function: <code class="language-plaintext highlighter-rouge">probs - labels_res</code>.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nd">@jax.custom_vjp</span>
<span class="k">def</span> <span class="nf">softmax_cross_entropy</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">label</span><span class="p">):</span>
    <span class="c1"># This is the forward definition of the function
</span>    <span class="n">max_logit</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">max</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span>
    <span class="n">exps</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">logits</span> <span class="o">-</span> <span class="n">max_logit</span><span class="p">)</span>
    <span class="n">probs</span> <span class="o">=</span> <span class="n">exps</span> <span class="o">/</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">exps</span><span class="p">)</span>
    <span class="k">return</span> <span class="o">-</span><span class="n">jnp</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">label</span> <span class="o">*</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="n">probs</span> <span class="o">+</span> <span class="mf">1e-7</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">softmax_cross_entropy_fwd</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">label</span><span class="p">):</span>
    <span class="c1"># Forward pass for VJP: compute output and save residuals
</span>    <span class="n">loss</span> <span class="o">=</span> <span class="nf">softmax_cross_entropy</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span>
    <span class="c1"># Re-compute probabilities here to save them as residuals for the backward pass.
</span>    <span class="n">max_logit</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">max</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span>
    <span class="n">exps</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">logits</span> <span class="o">-</span> <span class="n">max_logit</span><span class="p">)</span>
    <span class="n">probs</span> <span class="o">=</span> <span class="n">exps</span> <span class="o">/</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">exps</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">loss</span><span class="p">,</span> <span class="p">(</span><span class="n">probs</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">softmax_cross_entropy_bwd</span><span class="p">(</span><span class="n">res</span><span class="p">,</span> <span class="n">g</span><span class="p">):</span>
    <span class="c1"># Backward pass for VJP
</span>    <span class="n">probs</span><span class="p">,</span> <span class="n">label_res</span> <span class="o">=</span> <span class="n">res</span>
    <span class="c1"># Gradient with respect to logits
</span>    <span class="n">grad_logits</span> <span class="o">=</span> <span class="n">g</span> <span class="o">*</span> <span class="p">(</span><span class="n">probs</span> <span class="o">-</span> <span class="n">label_res</span><span class="p">)</span>
    <span class="c1"># The gradient for 'label' is not needed, so we return None.
</span>    <span class="c1"># The returned tuple must match the number of inputs to the function.
</span>    <span class="nf">return </span><span class="p">(</span><span class="n">grad_logits</span><span class="p">,</span> <span class="bp">None</span><span class="p">)</span>

<span class="n">softmax_cross_entropy</span><span class="p">.</span><span class="nf">defvjp</span><span class="p">(</span><span class="n">softmax_cross_entropy_fwd</span><span class="p">,</span> <span class="n">softmax_cross_entropy_bwd</span><span class="p">)</span>
</code></pre></div></div> <h2 id="putting-it-all-together-a-full-example">Putting It All Together: A Full Example</h2> <p>Now, I will combine all the pieces together to define the two-layer neural network and train it on the MNIST dataset using JAX with our custom backpropagation implementations in a single script of 150 lines.</p> <h3 id="training-a-neural-network-on-mnist">Training a Neural Network on MNIST</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">jax</span>
<span class="kn">import</span> <span class="n">jax.numpy</span> <span class="k">as</span> <span class="n">jnp</span>
<span class="kn">from</span> <span class="n">jax</span> <span class="kn">import</span> <span class="n">grad</span><span class="p">,</span> <span class="n">jit</span><span class="p">,</span> <span class="n">value_and_grad</span><span class="p">,</span> <span class="n">vmap</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="kn">import</span> <span class="n">torch</span>
<span class="kn">from</span> <span class="n">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>
<span class="kn">from</span> <span class="n">torchvision.datasets</span> <span class="kn">import</span> <span class="n">MNIST</span>
<span class="kn">from</span> <span class="n">torchvision</span> <span class="kn">import</span> <span class="n">transforms</span>


<span class="c1"># 1. Linear Layer
</span><span class="nd">@jax.custom_vjp</span>
<span class="k">def</span> <span class="nf">Linear</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">W</span> <span class="o">@</span> <span class="n">x</span>

<span class="k">def</span> <span class="nf">Linear_fwd</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="nc">Linear</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">x</span><span class="p">),</span> <span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">Linear_bwd</span><span class="p">(</span><span class="n">res</span><span class="p">,</span> <span class="n">g</span><span class="p">):</span>
    <span class="n">W</span><span class="p">,</span> <span class="n">x</span> <span class="o">=</span> <span class="n">res</span>
    <span class="n">grad_W</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">outer</span><span class="p">(</span><span class="n">g</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
    <span class="n">grad_x</span> <span class="o">=</span> <span class="n">W</span><span class="p">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">g</span>
    <span class="nf">return </span><span class="p">(</span><span class="n">grad_W</span><span class="p">,</span> <span class="n">grad_x</span><span class="p">)</span>

<span class="n">Linear</span><span class="p">.</span><span class="nf">defvjp</span><span class="p">(</span><span class="n">Linear_fwd</span><span class="p">,</span> <span class="n">Linear_bwd</span><span class="p">)</span>

<span class="nd">@jax.custom_vjp</span>
<span class="k">def</span> <span class="nf">Tanh</span><span class="p">(</span><span class="n">z</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">tanh</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">Tanh_fwd</span><span class="p">(</span><span class="n">z</span><span class="p">):</span>
    <span class="n">a</span> <span class="o">=</span> <span class="nc">Tanh</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">a</span><span class="p">,</span> <span class="p">(</span><span class="n">a</span><span class="p">,)</span>

<span class="k">def</span> <span class="nf">Tanh_bwd</span><span class="p">(</span><span class="n">res</span><span class="p">,</span> <span class="n">g</span><span class="p">):</span>
    <span class="n">a</span><span class="p">,</span> <span class="o">=</span> <span class="n">res</span>
    <span class="n">grad_z</span> <span class="o">=</span> <span class="n">g</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">a</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
    <span class="nf">return </span><span class="p">(</span><span class="n">grad_z</span><span class="p">,)</span>

<span class="n">Tanh</span><span class="p">.</span><span class="nf">defvjp</span><span class="p">(</span><span class="n">Tanh_fwd</span><span class="p">,</span> <span class="n">Tanh_bwd</span><span class="p">)</span>

<span class="nd">@jax.custom_vjp</span>
<span class="k">def</span> <span class="nf">softmax_cross_entropy</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">label</span><span class="p">):</span>
    <span class="n">max_logit</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">max</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span>
    <span class="n">exps</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">logits</span> <span class="o">-</span> <span class="n">max_logit</span><span class="p">)</span>
    <span class="n">probs</span> <span class="o">=</span> <span class="n">exps</span> <span class="o">/</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">exps</span><span class="p">)</span>
    <span class="k">return</span> <span class="o">-</span><span class="n">jnp</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">label</span> <span class="o">*</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="n">probs</span> <span class="o">+</span> <span class="mf">1e-7</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">softmax_cross_entropy_fwd</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">label</span><span class="p">):</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="nf">softmax_cross_entropy</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span>
    <span class="c1"># Re-compute probabilities here to save them as residuals for the backward pass.
</span>    <span class="n">max_logit</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">max</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span>
    <span class="n">exps</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">logits</span> <span class="o">-</span> <span class="n">max_logit</span><span class="p">)</span>
    <span class="n">probs</span> <span class="o">=</span> <span class="n">exps</span> <span class="o">/</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">exps</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">loss</span><span class="p">,</span> <span class="p">(</span><span class="n">probs</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">softmax_cross_entropy_bwd</span><span class="p">(</span><span class="n">res</span><span class="p">,</span> <span class="n">g</span><span class="p">):</span>
    <span class="n">probs</span><span class="p">,</span> <span class="n">label_res</span> <span class="o">=</span> <span class="n">res</span>
    <span class="n">grad_logits</span> <span class="o">=</span> <span class="n">g</span> <span class="o">*</span> <span class="p">(</span><span class="n">probs</span> <span class="o">-</span> <span class="n">label_res</span><span class="p">)</span>
    <span class="c1"># The gradient for 'label' is not needed, so we return None.
</span>    <span class="c1"># The returned tuple must match the number of inputs to the function.
</span>    <span class="nf">return </span><span class="p">(</span><span class="n">grad_logits</span><span class="p">,</span> <span class="bp">None</span><span class="p">)</span>

<span class="n">softmax_cross_entropy</span><span class="p">.</span><span class="nf">defvjp</span><span class="p">(</span><span class="n">softmax_cross_entropy_fwd</span><span class="p">,</span> <span class="n">softmax_cross_entropy_bwd</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">init_params</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">sizes</span><span class="p">):</span>
    <span class="n">keys</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="nf">len</span><span class="p">(</span><span class="n">sizes</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">jax</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">normal</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="p">(</span><span class="n">out_size</span><span class="p">,</span> <span class="n">in_size</span><span class="p">))</span> <span class="o">*</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="mf">2.</span> <span class="o">/</span> <span class="n">in_size</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">in_size</span><span class="p">,</span> <span class="n">out_size</span> <span class="ow">in</span> <span class="nf">zip</span><span class="p">(</span><span class="n">keys</span><span class="p">,</span> <span class="n">sizes</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">sizes</span><span class="p">[</span><span class="mi">1</span><span class="p">:])]</span>

<span class="k">def</span> <span class="nf">forward_pass</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">image</span><span class="p">):</span>
    <span class="n">W1</span><span class="p">,</span> <span class="n">W2</span> <span class="o">=</span> <span class="n">params</span>
    <span class="n">z1</span> <span class="o">=</span> <span class="nc">Linear</span><span class="p">(</span><span class="n">W1</span><span class="p">,</span> <span class="n">image</span><span class="p">)</span>
    <span class="n">a1</span> <span class="o">=</span> <span class="nc">Tanh</span><span class="p">(</span><span class="n">z1</span><span class="p">)</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="nc">Linear</span><span class="p">(</span><span class="n">W2</span><span class="p">,</span> <span class="n">a1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">logits</span>

<span class="c1"># Vectorize the forward pass to handle batches efficiently
</span><span class="n">batch_forward</span> <span class="o">=</span> <span class="nf">vmap</span><span class="p">(</span><span class="n">forward_pass</span><span class="p">,</span> <span class="n">in_axes</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">batch_loss_fn</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">images</span><span class="p">,</span> <span class="n">labels</span><span class="p">):</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="nf">batch_forward</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">images</span><span class="p">)</span>
    <span class="c1"># Vmap the single-example loss function over the batch
</span>    <span class="n">losses</span> <span class="o">=</span> <span class="nf">vmap</span><span class="p">(</span><span class="n">softmax_cross_entropy</span><span class="p">)(</span><span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">losses</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">accuracy</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">images</span><span class="p">,</span> <span class="n">labels</span><span class="p">):</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="nf">batch_forward</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">images</span><span class="p">)</span>
    <span class="n">predicted_class</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">true_class</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">predicted_class</span> <span class="o">==</span> <span class="n">true_class</span><span class="p">)</span>

<span class="nd">@jit</span>
<span class="k">def</span> <span class="nf">update_step</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">images</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">):</span>
    <span class="n">loss</span><span class="p">,</span> <span class="n">grads</span> <span class="o">=</span> <span class="nf">value_and_grad</span><span class="p">(</span><span class="n">batch_loss_fn</span><span class="p">)(</span><span class="n">params</span><span class="p">,</span> <span class="n">images</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
    <span class="n">updated_params</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">tree_util</span><span class="p">.</span><span class="nf">tree_map</span><span class="p">(</span>
        <span class="k">lambda</span> <span class="n">p</span><span class="p">,</span> <span class="n">g</span><span class="p">:</span> <span class="n">p</span> <span class="o">-</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">g</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">grads</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">updated_params</span><span class="p">,</span> <span class="n">loss</span>

<span class="c1"># --- PyTorch Data Loading and Training Loop ---
</span>
<span class="k">def</span> <span class="nf">load_mnist_data_pytorch</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">128</span><span class="p">):</span>
    <span class="n">transform</span> <span class="o">=</span> <span class="n">transforms</span><span class="p">.</span><span class="nc">Compose</span><span class="p">([</span>
        <span class="n">transforms</span><span class="p">.</span><span class="nc">ToTensor</span><span class="p">(),</span>
        <span class="n">transforms</span><span class="p">.</span><span class="nc">Normalize</span><span class="p">((</span><span class="mf">0.1307</span><span class="p">,),</span> <span class="p">(</span><span class="mf">0.3081</span><span class="p">,))</span>
    <span class="p">])</span>
    <span class="n">train_dataset</span> <span class="o">=</span> <span class="nc">MNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="sh">'</span><span class="s">./data</span><span class="sh">'</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">)</span>
    <span class="n">test_dataset</span> <span class="o">=</span> <span class="nc">MNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="sh">'</span><span class="s">./data</span><span class="sh">'</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">)</span>

    <span class="n">train_loader</span> <span class="o">=</span> <span class="nc">DataLoader</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">test_loader</span> <span class="o">=</span> <span class="nc">DataLoader</span><span class="p">(</span><span class="n">test_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">train_loader</span><span class="p">,</span> <span class="n">test_loader</span>

<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="sh">'</span><span class="s">__main__</span><span class="sh">'</span><span class="p">:</span>
    <span class="n">train_loader</span><span class="p">,</span> <span class="n">test_loader</span> <span class="o">=</span> <span class="nf">load_mnist_data_pytorch</span><span class="p">()</span>

    <span class="c1"># Network architecture: 784 -&gt; 512 -&gt; 10
</span>    <span class="n">layer_sizes</span> <span class="o">=</span> <span class="p">[</span><span class="mi">784</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="mi">10</span><span class="p">]</span>
    <span class="n">key</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nc">PRNGKey</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">params</span> <span class="o">=</span> <span class="nf">init_params</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">layer_sizes</span><span class="p">)</span>

    <span class="n">epochs</span> <span class="o">=</span> <span class="mi">10</span>
    <span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.01</span>

    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Starting training...</span><span class="sh">"</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
        <span class="n">epoch_loss</span> <span class="o">=</span> <span class="mf">0.0</span>
        <span class="n">num_batches</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="ow">in</span> <span class="n">train_loader</span><span class="p">:</span>
            <span class="n">images_np</span> <span class="o">=</span> <span class="n">images</span><span class="p">.</span><span class="nf">numpy</span><span class="p">()</span>
            <span class="n">labels_np</span> <span class="o">=</span> <span class="n">labels</span><span class="p">.</span><span class="nf">numpy</span><span class="p">()</span>

            <span class="n">images_np</span> <span class="o">=</span> <span class="n">images_np</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="n">images_np</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">labels_one_hot</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nf">one_hot</span><span class="p">(</span><span class="n">labels_np</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>

            <span class="n">params</span><span class="p">,</span> <span class="n">loss_val</span> <span class="o">=</span> <span class="nf">update_step</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">images_np</span><span class="p">,</span> <span class="n">labels_one_hot</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">)</span>
            <span class="n">epoch_loss</span> <span class="o">+=</span> <span class="n">loss_val</span>
            <span class="n">num_batches</span> <span class="o">+=</span> <span class="mi">1</span>

        <span class="n">avg_loss</span> <span class="o">=</span> <span class="n">epoch_loss</span> <span class="o">/</span> <span class="n">num_batches</span>

        <span class="n">test_images_tensor</span><span class="p">,</span> <span class="n">test_labels_tensor</span> <span class="o">=</span> <span class="nf">next</span><span class="p">(</span><span class="nf">iter</span><span class="p">(</span><span class="n">test_loader</span><span class="p">))</span>
        <span class="n">test_images_np</span> <span class="o">=</span> <span class="n">test_images_tensor</span><span class="p">.</span><span class="nf">numpy</span><span class="p">().</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">784</span><span class="p">)</span>
        <span class="n">test_labels_one_hot</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nf">one_hot</span><span class="p">(</span><span class="n">test_labels_tensor</span><span class="p">.</span><span class="nf">numpy</span><span class="p">(),</span> <span class="mi">10</span><span class="p">)</span>

        <span class="n">test_acc</span> <span class="o">=</span> <span class="nf">accuracy</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">test_images_np</span><span class="p">,</span> <span class="n">test_labels_one_hot</span><span class="p">)</span>

        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Epoch </span><span class="si">{</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="si">}</span><span class="s">/</span><span class="si">{</span><span class="n">epochs</span><span class="si">}</span><span class="s"> | Loss: </span><span class="si">{</span><span class="n">avg_loss</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="s"> | Test Accuracy: </span><span class="si">{</span><span class="n">test_acc</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <h3 id="results-and-verification">Results and Verification</h3> <p>Let’s see if our definitions worked as expected by running the training script:</p> <div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Starting training...
Epoch 1/10 | Loss: 0.5898 | Test Accuracy: 0.9009
Epoch 2/10 | Loss: 0.3269 | Test Accuracy: 0.9187
Epoch 3/10 | Loss: 0.2807 | Test Accuracy: 0.9262
Epoch 4/10 | Loss: 0.2536 | Test Accuracy: 0.9323
Epoch 5/10 | Loss: 0.2341 | Test Accuracy: 0.9363
Epoch 6/10 | Loss: 0.2186 | Test Accuracy: 0.9387
Epoch 7/10 | Loss: 0.2056 | Test Accuracy: 0.9428
Epoch 8/10 | Loss: 0.1943 | Test Accuracy: 0.9448
Epoch 9/10 | Loss: 0.1844 | Test Accuracy: 0.9466
Epoch 10/10 | Loss: 0.1755 | Test Accuracy: 0.9487
</code></pre></div></div> <p>The model achieves over 94% accuracy on the MNIST test set after just 10 epochs of training, demonstrating that our custom backpropagation implementations are functioning correctly!</p> <p>Let’s visualize some predictions from the trained model to see how well it performs.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/inline/backprop-mnist-480.webp 480w,/assets/img/inline/backprop-mnist-800.webp 800w,/assets/img/inline/backprop-mnist-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/inline/backprop-mnist.png" class="img-fluid rounded d-block mx-auto" width="900px" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">The model's predictions on some MNIST test images.</figcaption> </figure>]]></content><author><name></name></author><category term="deep learning"/><category term="JAX"/><summary type="html"><![CDATA[A deep dive into the backpropagation algorithm]]></summary></entry><entry><title type="html">Sui Generis (ERC)</title><link href="https://0xgeorgeassaad.github.io/ERC/" rel="alternate" type="text/html" title="Sui Generis (ERC)"/><published>2025-06-22T00:00:00+00:00</published><updated>2025-06-22T00:00:00+00:00</updated><id>https://0xgeorgeassaad.github.io/ERC</id><content type="html" xml:base="https://0xgeorgeassaad.github.io/ERC/"><![CDATA[<p>I was fortunate enough to be awarded this amazing scholarship, and today I want to share a little of my experience with <strong><em>you</em></strong>, the upcoming recipients. I am a big critic of the notion that we all share the same underlying experience. I am adamant that my experience was unique, and yours will be too. That’s why I titled this blog “Sui Generis”, Latin for “of its own kind.” But even though my experience is unique, there are always lessons worth sharing that you might learn from. To be clear, I don’t consider myself worthy of giving advice. So, the following are not rules, but rather pointers or hints from my own journey that you might find handy.</p> <h3 id="make-long-lasting-friendships">Make <em>Long-Lasting</em> Friendships</h3> <p>First, I want to give you a glimpse of a day in my life as an undergraduate. I would wake up at 5:20 AM, having slept only about six hours. I lived about half an hour from the bus pickup point and needed to be there by 6:40 AM. From the moment I left my house, it took about two hours to reach the university. In total, that was four hours of commuting every day.</p> <p>This sounds brutal, but I genuinely didn’t mind the time I spent on the bus. This was because I always had the most interesting conversations about the most random topics with my dear friend, Mostafa. The crazy thing is that I even grew to enjoy my commute. I looked forward to those hours of discussion more than anything else in my day. Things that seem unbearable from the outside can pass smoothly or even become entertaining when you have the right company.</p> <p>This friendship even led to some unexpected adventures. For some reason, I had a running joke with my friends about the German Bundestag. So when I later visited Germany, I went out of my way to go to Berlin just to take a photo in front of the Bundestag. Here is an image of my very happy self in front of it<sup id="fnref:1"><a href="#fn:1" class="footnote" rel="footnote" role="doc-noteref">1</a></sup>. You never know; sometimes, the jokes you share with friends can turn into reality.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/inline/Me_Bundestag-480.webp 480w,/assets/img/inline/Me_Bundestag-800.webp 800w,/assets/img/inline/Me_Bundestag-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/inline/Me_Bundestag.jpeg" class="img-fluid rounded z-depth-1 d-block mx-auto" width="500px" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">In front of the German Bundestag.</figcaption> </figure> <p>I want to put great emphasis on “long-lasting.” You want to find friends you will still be talking to five, ten, or even fifteen years from now. Having no friends is better than having friendships that are transitory or superficial. Always look for quality, not quantity. It’s far superior to have a handful of deep, perpetual friendships than a huge number of shallow, transactional ones.</p> <h3 id="have-fun">Have Fun</h3> <p>This is one of those things we constantly need to be reminded of. We, myself included, often forget one of life’s greatest hacks: having fun. We get so consumed by where we are, where we want to go, and, mistakenly, where others are, that we completely lose the plot.</p> <p>You need to put a little extra effort into everything you do to ensure a fun element is embedded within it. I am telling you, you can absolutely have fun doing anything, even the most tedious tasks like completing an assignment or finishing a semester project. You just have to be mindful about it. I had the most fun while finishing a project just hours before the deadline or studying for an exam on the day it was scheduled<sup id="fnref:2"><a href="#fn:2" class="footnote" rel="footnote" role="doc-noteref">2</a></sup>. Sometimes, a little adrenaline kick is all you need.</p> <h3 id="savor-every-moment">Savor Every Moment</h3> <p>Anyone working in the field of AI knows that we continuously move the goalposts. Once we built models that could perform classification and regression better than humans, we said, “This isn’t creative.” We needed models that could generate content like images or sound. Once we had these generative models, we thought we could rest assured and enjoy the win. Funnily enough, we now have models that can generate almost any type of content, but the goal has shifted again to Artificial General Intelligence (AGI). You can see the pattern here. We are trapped in an endless cycle; no matter what milestone we achieve, we always feel we are behind. We just need one more thing.</p> <p>This isn’t unique to the AI field. We are all victims of this mindset. Appreciate every single moment of your undergraduate journey to the fullest. If you are solving a challenging assignment, enjoy the struggle. If you are applying for that internship you’ve always wanted, enjoy the process and the anticipation. If this is your first semester and you are frustrated by the new system, enjoy the novelty. Let me tell you, even in the most difficult moments, there is always something to delight in. So, <em>savor every moment</em>.</p> <hr/> <h4 id="addendum"><strong>Addendum</strong></h4> <p>I didn’t talk about courses, exams, or anything related to studying because I believe those are the easiest things to figure out. You will become a master at these monotonous tasks after just a semester or two. Most importantly, keep your eye on your north star and follow it religiously. You will eventually graduate, but it’s the direction that truly matters.</p> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:1"> <p>Sadly, the Bundestag was being renovated at the time. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:2"> <p>This was quite rare, but when it did happen, it was super exciting. <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> </ol> </div>]]></content><author><name></name></author><category term="misc"/><summary type="html"><![CDATA[Sui Generis (ERC)]]></summary></entry><entry><title type="html">Linear Algebra Part 01: Identities</title><link href="https://0xgeorgeassaad.github.io/blog/2024/LA-01/" rel="alternate" type="text/html" title="Linear Algebra Part 01: Identities"/><published>2024-07-19T00:00:00+00:00</published><updated>2024-07-19T00:00:00+00:00</updated><id>https://0xgeorgeassaad.github.io/blog/2024/LA-01</id><content type="html" xml:base="https://0xgeorgeassaad.github.io/blog/2024/LA-01/"><![CDATA[<style>.post-content details{color:var(--global-text-color);background-color:var(--global-code-bg-color);margin-top:0;padding:8px 12px;position:relative;border-radius:6px;display:block;margin-bottom:20px;max-width:100%;overflow:auto}.post-content details summary{color:var(--global-theme-color);cursor:pointer;font-weight:500}.post-content details p{margin-top:.5rem;margin-bottom:.5rem}</style> <p>Linear Algebra is a fascinating and foundational subject in mathematics. In this post, we are going to explore some of its most important identities with proofs and also shed light on some interesting and closely related results.</p> <h3 id="definitions-of-scalars-vector-and-matrices">Definitions of Scalars, Vector, and Matrices</h3> <ol> <li>Scalars: Scalars are numerical values from $\mathbb{R}$</li> <li>Vectors: Vectors are arrays of scalars where one of the dimensions is 1. A $1 \times d$ is called <em>a row vector</em>, whereas a $d \times 1$ is called a column vector. The entries in a vector are called <em>“components”</em>.</li> <li>Matrices: Matrices are the logical extension of vectors and can be thought of as rectangular arrays of scalars. To get an element inside the matrix, one has to define the row $i$ and column $j$ to obtain element $a_{ij}$ from matrix $A = [a_{ij}]_{n \times d}$</li> </ol> <h3 id="operations-with-scalars-and-vectors">Operations with Scalars and Vectors</h3> <p>Let 2 $d-$dimensional vectors be $\bar x = [x_1 \dots x_d]$ and $\bar y = [y_1 \dots y_d]$. They can be added component-wise as such:</p> \[\bar x + \bar y = [x_1 + y_1 \dots x_d + y_d]\] <p>and subtracted in the same way</p> \[\bar x - \bar y = [x_1 - y_1 \dots x_d - y_d]\] <p>A scalar $a$ can be multiplied by vector as such:</p> \[a\bar x = [ax_1 \dots ax_d]\] <p>The <em>dot product</em> is defined as the sum of the component-wise multiplication:</p> \[\bar x \cdot \bar y = \sum_{i=1}^{d} x_i y_i\] <p>The Euclidean norm is defined in terms of the dot product</p> \[\lVert \bar x \rVert^2 = \bar x \cdot \bar x = \sum_{i=1}^{d} x_i^2\] <p>The Euclidean norm is called the $L_2$-norm which can be generalized to any $L_p$-norm as follows</p> \[\lVert \bar x \rVert_p = \bar x \cdot \bar x = \left( \sum_{i=1}^{d} |x_i|^p \right)^{(1/p)}\] <p>The dot product satisfies the <em>Cauchy-Schwarz inequality</em> which sets an upper bound for the value of the dot product</p> \[\lvert \bar x \cdot \bar y \rvert \leq \lVert \bar x \rVert \lVert \bar y \rVert\] <p><strong>Proof:</strong> Let $\bar x^\prime$ and $\bar y^\prime$ be unit vectors constructed from $\bar x$ and $\bar y$ such that</p> \[\bar x^\prime = \dfrac{\bar x}{\lVert \bar x \rVert}\] <p>and</p> \[\bar y^\prime = \dfrac{\bar y}{\lVert \bar y \rVert}\] <p>We show that $\lvert \bar x^\prime \cdot \bar y^\prime \rvert \leq 1$ by calculating $\lVert \bar x^\prime - \bar y^\prime \rVert^2$ and $\lVert \bar x^\prime + \bar y^\prime \rVert^2$ as such</p> \[\begin{aligned} \lVert \bar x^\prime - \bar y^\prime \rVert^2 &amp;= (\bar x^\prime - \bar y^\prime) \cdot (\bar x^\prime - \bar y^\prime) = \lVert \bar x^\prime \rVert + \lVert \bar y^\prime \rVert - 2 \bar x^\prime \cdot \bar y^\prime = 2 - 2 \bar x^\prime \cdot \bar y^\prime \\\\ \lVert \bar x^\prime + \bar y^\prime \rVert^2 &amp;= (\bar x^\prime + \bar y^\prime) \cdot (\bar x^\prime + \bar y^\prime) = \lVert \bar x^\prime \rVert + \lVert \bar y^\prime \rVert + 2 \bar x^\prime \cdot \bar y^\prime = 2 + 2 \bar x^\prime \cdot \bar y^\prime \end{aligned}\] <p>Since both $\lVert \bar x^\prime - \bar y^\prime \rVert^2$ and $\lVert \bar x^\prime + \bar y^\prime \rVert^2$ are nonnegative, then $\lvert \bar x^\prime \cdot \bar y^\prime \rvert \leq 1$</p> <p>Now, we scale the unit vectors to obtain the general inequality</p> \[\begin{aligned} \lvert \bar x^\prime \cdot \bar y^\prime \rvert &amp;\leq 1 \\\\ \lVert \bar x \rVert \lVert \bar y \rVert \lvert \bar x^\prime \cdot \bar y^\prime \rvert &amp;\leq \lVert \bar x \rVert \lVert \bar y \rVert \\\\ \lvert \bar x \cdot \bar y \rvert &amp;\leq \lVert \bar x \rVert \lVert \bar y \rVert \end{aligned}\] <p>Using the <em>Cauchy-Schwarz inequality</em>, we can also prove the triangle inequality which states that, in a triangle formed by $\bar x$ and $\bar y$, the side length of $\lVert \bar x - \bar y \rVert$ is no longer than the sum of the two other sides</p> \[\lVert \bar x - \bar y \rVert \leq \lVert \bar x \rVert + \lVert \bar y \rVert\] <p><strong>Proof:</strong> Since both sides of the triangle inequality are nonnegative, we only need to show that it still holds after squaring both sides.</p> \[\begin{aligned} \lvert \bar x \cdot \bar y \rvert &amp;\leq \lVert \bar x \rVert \lVert \bar y \rVert \\ -\lVert \bar x \rVert \lVert \bar y \rVert &amp;\leq \bar x \cdot \bar y \leq \lVert \bar x \rVert \lVert \bar y \rVert \\ \bar x \cdot \bar y &amp;\geq - \lVert \bar x \rVert \lVert \bar y \rVert \\ -2\bar x \cdot \bar y &amp;\leq 2 \lVert \bar x \rVert \lVert \bar y \rVert \\ \lVert \bar x \rVert^2 + \lVert \bar y \rVert^2 -2\bar x \cdot \bar y &amp;\leq \lVert \bar x \rVert^2 + \lVert \bar y \rVert^2+ 2 \lVert \bar x \rVert \lVert \bar y \rVert \\ (\bar x - \bar y) \cdot (\bar x - \bar y) &amp;\leq (\lVert \bar x \rVert +\lVert \bar y \rVert)^2 \\ (\lVert \bar x - \bar y \rVert)^2 &amp;\leq (\lVert \bar x \rVert + \lVert \bar y \rVert)^2 \end{aligned}\] <h3 id="operations-with-vectors-and-matrices">Operations with Vectors and Matrices</h3> <p>The transpose of a matrix is obtained by flipping its rows and columns such that the $(i, j)th$ entry of the transpose is the same as the $(j, i)th$ entry of the original matrix. Therefore, the transpose of an $n \times d$ matrix is a $d \times n$ matrix. The transpose of a matrix $A$ is denoted by $A^T$. An example of the transpose of $A$ is shown:</p> \[\begin{bmatrix} a_{11} &amp; a_{12} \\ a_{21} &amp; a_{22} \end{bmatrix}^T = \begin{bmatrix} a_{11} &amp; a_{21} \\ a_{12} &amp; a_{22} \end{bmatrix}\] <p>It immediately follows that $(A^T)^T = A$ and that the transpose of the sum is the sum of the transpose:</p> \[(A+B)^T = A^T + B^T\] <p>For the matrix-vector product of $n \times d$ matrix $A$ with $d \times 1$ vector $\bar x$, we can observe that the elements of $\bar x$ act as weights for each column of $A$</p> \[\begin{bmatrix} a_{11} &amp; a_{12} \\ a_{21} &amp; a_{22} \\ a_{31} &amp; a_{32} \end{bmatrix} \begin{bmatrix} x_1 \\ x_2 \end{bmatrix} = \begin{bmatrix} x_1 a_{11} + x_2 a_{12} \\ x_1 a_{21} + x_2 a_{22} \\ x_1 a_{31} + x_2 a_{32} \end{bmatrix}\] <p>As a general form: $ A \bar x = \sum_{i=0}^{d} x_i \bar a_i$</p> <p>Now, we define outer products which can be performed between vectors of different lengths</p> \[\bar x \bar y ^T = \begin{bmatrix} x_1 \\ x_2 \\ x_3 \end{bmatrix} \begin{bmatrix} y_1 &amp; y_2 &amp; y_3 \end{bmatrix}= \begin{bmatrix} y_1x_1 &amp; y_2x_1 &amp; y_3x_1 \\ y_1x_2 &amp; y_2x_2 &amp; y_3x_2 \\ y_1x_3 &amp; y_2x_3 &amp; y_3x_3 \\ \end{bmatrix}\] <p>We can also show that if the outer product between an $n\times1$ vector is and a $1\times d$ vector result in an $n\times d$ matrix with the following properties: (i) Every row is a multiple of every other row, and (ii) every column is a multiple of every other column:</p> \[\begin{bmatrix} a_1 \\ a_2 \\ \vdots \\ a_n \end{bmatrix}_{n \times 1} \begin{bmatrix} b_1 &amp; b_2 &amp; \dots &amp; b_d \end{bmatrix}_{1 \times d} = \begin{bmatrix} a_1b_1 &amp; a_1b_2 &amp; \dots &amp; a_1b_d \\ a_2b_1 &amp; a_2b_2 &amp; \dots &amp; a_2b_d \\ \vdots &amp; \ddots &amp; \ddots &amp; \vdots \\ a_nb_1 &amp; a_nb_2 &amp; \dots &amp; a_nb_d \\ \end{bmatrix}_{n \times d}\] <p>It is clear that each row $\bar R_i = \dfrac{\bar R_j a_i}{a_j}$ and that each column $\bar C_i = \dfrac{\bar C_j b_i}{b_j}$</p> <p><strong>Exercise:</strong> Let A be an $1000000 \times 2$ matrix. Suppose you have to compute the $2 \times 1000000$ matrix $A^T AA^T$ on a computer with limited memory. Would you prefer to compute $(A^T A)A^T$ or would you prefer to compute $A^T (AA^T)$?.</p> <details><summary>Click here for the answer.</summary> <p><strong>Sol:</strong></p> \[\begin{aligned} (A^T A)A^T &amp;\rightarrow (2 \times 2) (2 \times 1000000) \ \text{ [lower memory usage]} \\ A^T (AA^T) &amp;\rightarrow (2 \times 1000000) (1000000 \times 1000000) \end{aligned}\] </details> <p><strong>Exercise:</strong> Let $D$ be an $n \times d$ matrix for which each column sums to 0. Let $A$ be an arbitrary $d \times d$ matrix. Show that the sum of each column of $DA$ is also zero.</p> <details><summary>Click here for the answer.</summary> <p><strong>Sol:</strong></p> \[\begin{aligned} \sum_{i=1}^{n} (DA)_{ij} = \sum_{i=1}^{n} \sum_{k=1}^{d} D_{ik} A_{kj} = \sum_{k=1}^{d} A_{kj} \sum_{i=1}^{n} D_{ik} = 0 \end{aligned}\] </details> <p><strong>Exercise:</strong> Show that \((A_1A_2A_3 \dots A_n)^T = A_n^T A_{n-1}^T \dots A_{2}^T A_{1}^T\)</p> <div class="mb-4"> <details><summary>Click here for the answer.</summary> <p><strong>Sol:</strong></p> \[(A_1A_2A_3 \dots A_n)^T = A_n^T A_{n-2}^T (A_1A_2A_3 \dots A_{n-2})^T = A_n^T A_{n-1}^T \dots A_{2}^T A_{1}\] </details> </div> <p>A matrix is called symmetric if it is square and equal to its transpose \(A=A^T\)</p> <p><strong>Exercise:</strong> If A and B are symmetric matrices, then show that AB is symmetric if and only if AB = BA</p> <div class="distill-details-wrapper"> <details><summary>Click here for the answer.</summary> <p><strong>Sol:</strong></p> \[AB = (AB)^T = B^T A^T = BA\] </details> <div> </div></div>]]></content><author><name></name></author><category term="math"/><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">Reimplementing python’s heapq module</title><link href="https://0xgeorgeassaad.github.io/blog/2024/python-heapq/" rel="alternate" type="text/html" title="Reimplementing python’s heapq module"/><published>2024-05-28T00:00:00+00:00</published><updated>2024-05-28T00:00:00+00:00</updated><id>https://0xgeorgeassaad.github.io/blog/2024/python-heapq</id><content type="html" xml:base="https://0xgeorgeassaad.github.io/blog/2024/python-heapq/"><![CDATA[<h3 id="difference-between-priority-queue-and-heap">Difference between Priority Queue and Heap</h3> <p>It’s important to note the difference between Priority Queue and Heap. Priority Queue is an <em>abstract data-type</em> that supports a specified set of operations but can be implemented using various data structures.</p> <p>A priority queue is similar to a normal queue with the only difference being that the order in which elements are removed from the queue depends on their priority not their time of arrival (addition), and as such, the data inserted into a priority queue must be comparable so that they can be ordered from least to greatest or vice versa.</p> <p>A priority queue supports the following set of operations:</p> <ul> <li><code class="language-plaintext highlighter-rouge">is_empty()</code>: checks whether the queue has no elements</li> <li><code class="language-plaintext highlighter-rouge">add(elem, prio)</code>: adds an element with a specified priority to the queue</li> <li><code class="language-plaintext highlighter-rouge">poll()</code>: removes the element with highest priority from the queue</li> </ul> <p>A priority queue can be implemented using a list, but it will be very inefficient. That’s why we will implement it using a heap which is a tree-based data structure that satisfies the heap invariant<sup id="fnref:1"><a href="#fn:1" class="footnote" rel="footnote" role="doc-noteref">1</a></sup>.</p> <p>The heap invariant states that the value of the parent node is smaller than the value of its children node (in case of min heap).</p> <h3 id="representation-of-a-heap">Representation of a Heap</h3> <p>Since a heap is a tree, it can represented using a pointers approach by creating a <code class="language-plaintext highlighter-rouge">node</code> class with properties <code class="language-plaintext highlighter-rouge">leftChild</code> and <code class="language-plaintext highlighter-rouge">rightChild</code>, but there is a much better and simpler representation using a simple list. So, for any parent at index $i$, we can calculate the indices of children as such:</p> \[\begin{aligned} \text{Left Child index } &amp;= 2 i+ 1 \\\\ \text{Right Child index } &amp;= 2 i+ 2 \\\\ \end{aligned}\] <p>Here is an example for a heap represented using a list</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#    0
#   / \
#  2   3
# / \ / \
# 4 5 6 4
</span><span class="n">heap</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="mi">4</span><span class="p">]</span>
</code></pre></div></div> <h3 id="turning-a-min-heap-into-a-max-heap">Turning a Min Heap into a Max Heap</h3> <p>Python’s original <code class="language-plaintext highlighter-rouge">heapq</code> as well as our reimplementation supports only a min heap. So, how can we turn that into a max heap?</p> <p>Since all elements in the heap implement a comparator interface, we can simply negate the comparator which, in effect, will result in the parent being larger than or equal to its children.</p> <p>One nice trick can be used when the elements themselves are numbers or when the priority is a number, We simply multiply the number by $-1$ before insertion and then multiply it back by $-1$ after removal.</p> <h3 id="adding-elements-to-the-heap">Adding elements to the heap</h3> <p>We want to maintain the complete binary tree property to achieve the nice $O(\log(n))$ runtime. We can do this by inserting any new element at the bottom leftmost position and then swimming the element to maintain the heap invariant.</p> <p>Now, we discuss the sift up algorithm starting at index $i$:</p> <ol> <li>Set <code class="language-plaintext highlighter-rouge">k = i</code> and <code class="language-plaintext highlighter-rouge">parent = (k - 1) // 2</code></li> <li>If node is smaller than parent, <ul> <li>swap node $k$ with parent</li> <li>set <code class="language-plaintext highlighter-rouge">k = parent</code> and <code class="language-plaintext highlighter-rouge">parent = (k - 1) // 2</code></li> <li>return to step 1</li> </ul> </li> <li>else, terminate</li> </ol> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># O(log(n))
</span><span class="k">def</span> <span class="nf">heappush</span><span class="p">(</span><span class="n">heap</span><span class="p">,</span> <span class="n">elem</span><span class="p">):</span>
    <span class="n">heap</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">elem</span><span class="p">)</span>
    <span class="nf">swim</span><span class="p">(</span><span class="n">heap</span><span class="p">,</span> <span class="nf">len</span><span class="p">(</span><span class="n">heap</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">swim</span><span class="p">(</span><span class="n">heap</span><span class="p">,</span> <span class="n">k</span><span class="p">):</span>
    <span class="n">parent</span> <span class="o">=</span> <span class="p">(</span><span class="n">k</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="mi">2</span>
    <span class="k">while</span> <span class="n">k</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">heap</span><span class="p">[</span><span class="n">parent</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">heap</span><span class="p">[</span><span class="n">k</span><span class="p">]:</span>
        <span class="n">heap</span><span class="p">[</span><span class="n">k</span><span class="p">],</span> <span class="n">heap</span><span class="p">[</span><span class="n">parent</span><span class="p">]</span> <span class="o">=</span> <span class="n">heap</span><span class="p">[</span><span class="n">parent</span><span class="p">],</span> <span class="n">heap</span><span class="p">[</span><span class="n">k</span><span class="p">]</span>
        <span class="n">k</span> <span class="o">=</span> <span class="n">parent</span>
        <span class="n">parent</span> <span class="o">=</span> <span class="p">(</span><span class="n">k</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="mi">2</span>
</code></pre></div></div> <h3 id="removing-elements-from-the-heap">Removing elements from the heap</h3> <p>When polling, the root is always located at index $0$. So, we extract <code class="language-plaintext highlighter-rouge">heap[0]</code> and swap the root <code class="language-plaintext highlighter-rouge">heap[0]</code> with the bottom rightmost leaf located at the end of the list <code class="language-plaintext highlighter-rouge">heap[-1]</code>. Then, to maintain the heap invariant we sink the element location at the root starting at index $0$.</p> <p>Now, we discuss the sinking algorithm starting at index $i$:</p> <p>One important note is that when comparing the node with its children and in case of a tie between the left and right node, we always default to the left child.</p> <ol> <li>Set <code class="language-plaintext highlighter-rouge">k = i</code></li> <li>Set <code class="language-plaintext highlighter-rouge">left = 2*k + 1</code>, <code class="language-plaintext highlighter-rouge">right = 2*k + 2</code></li> <li>Set <code class="language-plaintext highlighter-rouge">smallest = left</code></li> <li>if the right element is smaller than the left element <ul> <li>Set <code class="language-plaintext highlighter-rouge">smallest = right</code></li> </ul> </li> <li>if $k$ is $\leq$ $parent$: <ul> <li>terminate</li> </ul> </li> <li>else, <ul> <li>swap node $k$ with smallest</li> <li>set <code class="language-plaintext highlighter-rouge">k = smallest</code></li> <li>return to step 2</li> </ul> </li> </ol> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># O(log(n))
</span><span class="k">def</span> <span class="nf">heappop</span><span class="p">(</span><span class="n">heap</span><span class="p">):</span>
    <span class="k">if</span> <span class="nf">len</span><span class="p">(</span><span class="n">heap</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">heap</span><span class="p">.</span><span class="nf">pop</span><span class="p">()</span>
    <span class="n">root</span> <span class="o">=</span> <span class="n">heap</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">heap</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">heap</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">heap</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">heap</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">heap</span><span class="p">.</span><span class="nf">pop</span><span class="p">()</span>
    <span class="nf">sink</span><span class="p">(</span><span class="n">heap</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">root</span>
<span class="k">def</span> <span class="nf">sink</span><span class="p">(</span><span class="n">heap</span><span class="p">,</span> <span class="n">k</span><span class="p">):</span>
    <span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
        <span class="n">left</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">k</span> <span class="o">+</span> <span class="mi">1</span>
        <span class="n">right</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">k</span> <span class="o">+</span> <span class="mi">2</span>
        <span class="n">smallest</span> <span class="o">=</span> <span class="n">left</span>
        <span class="k">if</span> <span class="n">right</span> <span class="o">&lt;</span> <span class="nf">len</span><span class="p">(</span><span class="n">heap</span><span class="p">)</span> <span class="ow">and</span> <span class="n">heap</span><span class="p">[</span><span class="n">right</span><span class="p">]</span> <span class="o">&lt;</span> <span class="n">heap</span><span class="p">[</span><span class="n">left</span><span class="p">]:</span>
            <span class="n">smallest</span> <span class="o">=</span> <span class="n">right</span>
        <span class="k">if</span> <span class="n">left</span> <span class="o">&gt;=</span> <span class="nf">len</span><span class="p">(</span><span class="n">heap</span><span class="p">)</span> <span class="ow">or</span> <span class="n">heap</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="n">heap</span><span class="p">[</span><span class="n">smallest</span><span class="p">]:</span>
            <span class="k">break</span>
        <span class="n">heap</span><span class="p">[</span><span class="n">k</span><span class="p">],</span> <span class="n">heap</span><span class="p">[</span><span class="n">smallest</span><span class="p">]</span> <span class="o">=</span> <span class="n">heap</span><span class="p">[</span><span class="n">smallest</span><span class="p">],</span> <span class="n">heap</span><span class="p">[</span><span class="n">k</span><span class="p">]</span>
        <span class="n">k</span> <span class="o">=</span> <span class="n">smallest</span>
</code></pre></div></div> <h3 id="heapifying-a-list">Heapifying a list</h3> <p>If we already have an unsorted list and we want to turn it into a heap, the naive approach would be creating a new heap and then <code class="language-plaintext highlighter-rouge">heappush</code> each element from the list, but that would result in a runtime of $O(n\log(n))$.</p> <p>There is a more efficient way that runs in a runtime of only $O(n)$. Now, the heapify algorithm is quite simple, but it’s quite hard to prove its runtime.</p> <p>If you are interested, you can find the proof here<sup id="fnref:2"><a href="#fn:2" class="footnote" rel="footnote" role="doc-noteref">2</a></sup>.</p> <p>We start from the last parent (located in the middle of the list) and then sink each parent down.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># O(n)
</span><span class="k">def</span> <span class="nf">heapify</span><span class="p">(</span><span class="n">heap</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">heap</span><span class="p">)</span> <span class="o">//</span> <span class="mi">2</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">):</span>
        <span class="nf">sink</span><span class="p">(</span><span class="n">heap</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span>
</code></pre></div></div> <h3 id="nlargest-or-nsmallest">nlargest (or nsmallest)</h3> <p>Now, imagine we want to get the $k$ largest elements from unsorted list of size $n$. The naive approach would be to first heapify the list and then pop $k$ elements from the newly built heap, but that would result in a runtime of $O(n)$ to heapify the list and a $O(\log(n))$ for each <code class="language-plaintext highlighter-rouge">heapop</code> operation resulting in a total runtime of $ O(n+ k\log(n))$ and $O(n)$ memory.</p> <p>There is another approach which is faster but might not be entirely clear to you at first. The key is to actually build a heap of only size $k$ and then traverse the rest of the list and only pop the root if the current array element <code class="language-plaintext highlighter-rouge">arr[i]</code> is larger. To see why at the end of traversal the heap would contain the $k$ largest elements, assume that the list was sorted. If we take the first $k$ elements and then heapify them, by looking at the $k+1$ element in the list, we immediately see that it’s larger than the root of the heap. Then, we pop the root and insert this new element. If we continue in this manner, we will have the $k$ largest elements by the end of the traversal. The last step would be sorting the heap.</p> <p>Now, Let’s see why it’s efficient.</p> <ul> <li>$O(\log(k))$: heapify first $k$ elements from the list</li> <li>$O((n-k) \log(k))$: at most we possibly would have to replace root of heap with each single element remaining in the list $(n-k)$ elements. (if the list was sorted)</li> <li>$O(k \log(k))$: sort final heap</li> </ul> <p>Thus, the total runtime is $ \log(k) + (n-k) \log(k)+ k \log(k) = (n+1)\log(k) \approx O(n\log(k))$ and $O(k)$ memory.</p> <p>We see that the second approach is better:</p> <ul> <li>memory: $O(k) &lt; O(n)$</li> <li>time: $O(n\log(k)) &lt; O(n+ k\log(n))$</li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># O(nlog(k))
</span><span class="k">def</span> <span class="nf">nlargest</span><span class="p">(</span><span class="n">arr</span><span class="p">,</span> <span class="n">k</span><span class="p">):</span>
    <span class="n">heap</span> <span class="o">=</span> <span class="n">arr</span><span class="p">[:</span><span class="n">k</span><span class="p">]</span>
    <span class="nf">heapify</span><span class="p">(</span><span class="n">heap</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="nf">len</span><span class="p">(</span><span class="n">arr</span><span class="p">)):</span>
        <span class="k">if</span> <span class="n">arr</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">heap</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
            <span class="nf">heappop</span><span class="p">(</span><span class="n">heap</span><span class="p">)</span>
            <span class="nf">heappush</span><span class="p">(</span><span class="n">heap</span><span class="p">,</span> <span class="n">arr</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
    <span class="n">heap</span><span class="p">.</span><span class="nf">sort</span><span class="p">(</span><span class="n">reverse</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">heap</span>
</code></pre></div></div> <h3 id="congrats">Congrats!!</h3> <p>Pat yourself on the back! We have just implemented the python’s <code class="language-plaintext highlighter-rouge">heapq</code> module. During writing, I didn’t look at the original source code, but if you are interested, here is the original source code of the <code class="language-plaintext highlighter-rouge">heapq</code> module<sup id="fnref:3"><a href="#fn:3" class="footnote" rel="footnote" role="doc-noteref">3</a></sup>. Just note that the implementation of python would be faster because it uses a byte-compiled version, but nonetheless you can look at the python source code to learn from it.</p> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:1"> <p><a href="https://en.wikipedia.org/wiki/Heap_(data_structure)">Heap (data structure) - Wikipedia</a> <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:2"> <p><a href="https://www.cs.umd.edu/~meesh/351/mount/lectures/lect14-heapsort-analysis-part.pdf">Heapify proof</a> <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:3"> <p><a href="https://github.com/python/cpython/blob/3.12/Lib/heapq.py">Python’s heaapq implementation</a> <a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> </ol> </div>]]></content><author><name></name></author><category term="code"/><summary type="html"><![CDATA[Difference between Priority Queue and Heap]]></summary></entry></feed>