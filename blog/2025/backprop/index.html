<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta name="google-site-verification" content="_QulXh9JitynR6CjGBqsroG6I5hve1zFQ3L7Jo_bf0o"> <meta http-equiv="Permissions-Policy" content="interest-cohort=()"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Everything You Need to Know About Backpropagation | George Assaad </title> <meta name="author" content="George Assaad"> <meta name="description" content="A deep dive into the backpropagation algorithm"> <meta name="keywords" content="George Assaad, deep learning, computer science, CS, mathematics, math, blog, front end, backend, CI-CD, CI/CD, React Native, AI, machine learning"> <meta property="og:site_name" content="George Assaad"> <meta property="og:type" content="article"> <meta property="og:title" content="George Assaad | Everything You Need to Know About Backpropagation"> <meta property="og:url" content="https://0xgeorgeassaad.github.io/blog/2025/backprop/"> <meta property="og:description" content="A deep dive into the backpropagation algorithm"> <meta property="og:image" content="/assets/img/cards/backprop-card.png"> <meta property="og:locale" content="en"> <meta name="twitter:card" content="summary"> <meta name="twitter:title" content="Everything You Need to Know About Backpropagation"> <meta name="twitter:description" content="A deep dive into the backpropagation algorithm"> <meta name="twitter:image" content="/assets/img/cards/backprop-card.png"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link rel="preconnect" href="https://fonts.googleapis.com"> <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin> <link href="https://fonts.googleapis.com/css2?family=Outfit:wght@100..900&amp;display=swap" rel="stylesheet"> <link href="https://fonts.googleapis.com/css2?family=Inter:ital,opsz,wght@0,14..32,100..900;1,14..32,100..900&amp;display=swap" rel="stylesheet"> <link href="https://fonts.googleapis.com/css2?family=JetBrains+Mono:ital,wght@0,100..800;1,100..800&amp;display=swap" rel="stylesheet"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?69aa09146a7f4a5237866bee4072efea" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/favicon.ico?0e253710211b0b96e987675d62c878a4"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://0xgeorgeassaad.github.io/blog/2025/backprop/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">George</span> Assaad </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Everything You Need to Know About Backpropagation</h1> <p class="post-meta"> Created on October 18, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/deep-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> deep learning</a>   <a href="/blog/tag/jax"> <i class="fa-solid fa-hashtag fa-sm"></i> JAX</a> </p> </header> <article class="post-content"> <div id="table-of-contents"> <ul id="toc" class="section-nav"> <li class="toc-entry toc-h2"><a href="#introduction">Introduction</a></li> <li class="toc-entry toc-h2"> <a href="#the-core-theory-of-backpropagation">The Core Theory of Backpropagation</a> <ul> <li class="toc-entry toc-h3"><a href="#the-chain-rule-with-vector-jacobian-products-vjp">The Chain Rule with Vector-Jacobian Products (VJP)</a></li> <li class="toc-entry toc-h3"><a href="#forward-vs-reverse-mode-why-backprop-wins">Forward vs. Reverse Mode: Why Backprop Wins</a></li> </ul> </li> <li class="toc-entry toc-h2"> <a href="#the-vjp-trick-never-materialize-jacobians">The VJP Trick: Never Materialize Jacobians</a> <ul> <li class="toc-entry toc-h3"><a href="#deriving-the-gradient-for-a-linear-layer">Deriving the Gradient for a Linear Layer</a></li> <li class="toc-entry toc-h3"><a href="#implementing-custom-vjps-in-jax">Implementing Custom VJPs in JAX</a></li> <li class="toc-entry toc-h3"><a href="#deriving-the-softmax-cross-entropy-gradient">Deriving the Softmax Cross-Entropy Gradient</a></li> </ul> </li> <li class="toc-entry toc-h2"> <a href="#putting-it-all-together-a-full-example">Putting It All Together: A Full Example</a> <ul> <li class="toc-entry toc-h3"><a href="#training-a-neural-network-on-mnist">Training a Neural Network on MNIST</a></li> <li class="toc-entry toc-h3"><a href="#results-and-verification">Results and Verification</a></li> </ul> </li> </ul> </div> <hr> <div id="markdown-content"> <h2 id="introduction">Introduction</h2> <p>No algorithm is more fundamental to deep learning than backpropagation. While the concept itself is beautifully simple, peeking under the hood of modern frameworks like PyTorch or TensorFlow reveals a labyrinth of device dispatchers, type handlers, and optimization layers that obscure the elegant mathematics beneath. Until JAX came along, which exposes a very elegant API for exploring and defining backprop operations very easily.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/cards/backprop-card-480.webp 480w,/assets/img/cards/backprop-card-800.webp 800w,/assets/img/cards/backprop-card-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/cards/backprop-card.png" class="img-fluid rounded d-block mx-auto" width="670px" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">The word 'Backprop' written using the same style as JAX logo</figcaption> </figure> <p>You might wonder: if modern frameworks handle differentiation automatically, why bother understanding backpropagation at all? The answer becomes clear the moment you need to push beyond standard operations. When optimizing critical bottlenecks with custom kernels in CUDA, Pallas (JAX), or Triton, you’re suddenly responsible for defining how gradients flow through your code. Without understanding backpropagation’s core mechanics (how to compute and propagate gradients correctly), you’ll struggle to write kernels that integrate seamlessly into your training loop. Even a foundational grasp of VJPs and the chain rule transforms your understanding of how deep learning models are trained.</p> <h2 id="the-core-theory-of-backpropagation">The Core Theory of Backpropagation</h2> <p>At its heart, a neural network is a series of nested mathematical functions. We start with an input, pass it through the first function (or “layer”) to get an intermediate result, pass that result through the next layer, and so on, until we get a final output. To “train” the network, we need to systematically adjust the parameters (the weights and biases) of each function to minimize the final error. Backpropagation is the clever and efficient algorithm that tells us exactly how to do this. It works by calculating the gradient, or the rate of change of the final error with respect to each parameter in the network, allowing an optimization algorithm like gradient descent to update the parameters in the right direction.</p> <p>To make this concrete, let’s trace the flow of data through a simple two-layer neural network, which will be our working example for the rest of this post.</p> \[\begin{align*} z_1 &amp;= f_1(x) = W_1 x \hspace{2cm} &amp;\text{// } x: (d_1, 1),\; W_1: (d_2, d_1),\; z_1: (d_2, 1) \\ a &amp;= \tanh(z_1) &amp;\text{// } a: (d_2, 1) \\z_2 &amp;= f_2(a) = W_2 a &amp;\text{// } W_2: (d_3, d_2),\; z_2: (d_3, 1) \\ y &amp;= \text{Softmax}(z_2) &amp;\text{// } y: (d_3, 1) \\ \mathcal{L} &amp;= \text{Loss}(y) &amp;\text{// } \mathcal{L}: \text{scalar} \end{align*}\] <h3 id="the-chain-rule-with-vector-jacobian-products-vjp">The Chain Rule with Vector-Jacobian Products (VJP)</h3> <p>This is the single most essential mathematical concept for understanding backpropagation:</p> <blockquote> <p><strong>Definition 1 (Vector-Jacobian Product)</strong> &gt; <br> <br> Let $y = f(x)$, $x \in \mathbb{R}^n, \quad y \in \mathbb{R}^m, \quad \mathcal{L} = \mathcal{L}(y)$ is a scalar loss function; then</p> \[\nabla_{x} \mathcal{L} = J_f(x)^T \nabla_{y} \mathcal{L}\] <p class="block-note">where $J_f(x) = \frac{\partial f(x)}{\partial x} \in \mathbb{R}^{m \times n}$.</p> </blockquote> <p>The key insight of backpropagation is understanding what the vector $\nabla_{y} \mathcal{L}$ represents. It is the “upstream gradient”, the gradient of the final scalar loss with respect to the output $y$ of the function $f$. The VJP $J_f(x)^T \nabla_{y} \mathcal{L}$ then gives us the “local gradient”, the gradient of the final scalar loss with respect to the function’s input $x$. Backpropagation is, in essence, a chain of these VJP calculations, passing the upstream gradient from the last layer all the way back to the first.</p> <p>Now, we can get the equation for computing the gradient of the loss with the weights of our furthest layer as such:</p> \[\begin{align*}\nabla_{z_2} \mathcal{L} &amp;= J_{y}^T \nabla_{y} \mathcal{L} \\\nabla_{a} \mathcal{L} &amp;= J_{z_2}^T \nabla_{z_2} \mathcal{L} = J_{z_2}^T J_{y}^T \nabla_{y} \mathcal{L} \\\nabla_{z_1} \mathcal{L} &amp;= J_{a}^T \nabla_{a} \mathcal{L}= J_{a}^T J_{z_2}^T J_{y}^T \nabla_{y} \mathcal{L} \\ \nabla_{W_1} \mathcal{L} &amp;= J_{z_1}^T \nabla_{z_1} \mathcal{L} = J_{z_1}^T J_{a}^T J_{z_2}^T J_{y}^T \nabla_{y} \mathcal{L} \end{align*}\] <p>It’s very important to take a moment and realize there are two ways to compute the gradient, one could start computing from left to right in a fashion known as <em>“Forward mode automatic differentiation”</em> or start computing from right to left in a fashion known as <em>“Backward mode automatic differentiation”</em>. The latter is the one used in training neural networks and commonly referred to as “backpropagation” or “backprop” for short.</p> \[\underbrace{\nabla_{W_1} \mathcal{L}}_{(d_2 \times d_1,\, d_2)} = \underbrace{J_{z_1}^T}_{(d_2 \times d_1,\, d_2)} \ \underbrace{J_{a}^T}_{(d_2,\, d_2)} \ \underbrace{J_{z_2}^T}_{(d_2,\, d_3)} \ \underbrace{J_{y}^T}_{(d_3,\, d_3)} \ \underbrace{\nabla_{y} \mathcal{L}}_{(d_3,\, 1)}\] <h3 id="forward-vs-reverse-mode-why-backprop-wins">Forward vs. Reverse Mode: Why Backprop Wins</h3> <p>To avoid any hand-waving, I won’t move on until I show you fully why backprop is a vastly more efficient method in training deep learning models compared to forward mode autodiff. (If you read this and aren’t yet convinced, leave a comment and I will try to provide another example).</p> <p>Even though in reality we need to compute $\nabla_{W_1} \mathcal{L}$, I will dissect the computation of $\nabla_{z_1} \mathcal{L}$ mainly because the leftmost jacobian turns out to be a simple outer product (we will show later that we never materialize any of the jacobians shown here).</p> \[\underbrace{\nabla_{z_1} \mathcal{L}}_{(d_2, 1)} = \underbrace{J_{a}^T}_{(d_2,\, d_2)} \ \underbrace{J_{z_2}^T}_{(d_2,\, d_3)} \ \underbrace{J_{y}^T}_{(d_3,\, d_3)} \ \underbrace{\nabla_{y} \mathcal{L}}_{(d_3,\, 1)}\] <p>In forward mode autodiff, we start from the left and compute all matrix multiplications as such:</p> \[\nabla_{z_1} \mathcal{L} = ((J_{a}^T J_{z_2}^T )J_{y}^T) \nabla_{y} \mathcal{L}\] <p>which yields the number of multiplications as follows (remember multiplying $(m,n) \times (n,p)$ matrices requires $mnp$ multiplications)</p> <p><strong>Total No. of Multiplications for Forward Mode</strong>: $d_2^2 d_3 + d_2 d_3^2 + d_2 d_3$</p> <p>In backward mode autodiff, we start from the right and compute all matrix multiplications as such:</p> \[\nabla_{z_1} \mathcal{L} = J_{a}^T (J_{z_2}^T (J_{y}^T \nabla_{y} \mathcal{L}))\] <p>which yields the number of multiplications as follows</p> <p><strong>Total No. of Multiplications for Reverse Mode</strong>: $d_3^2 + d_2 d_3 + d_2^2$</p> <p>The reason backpropagation is universally used for training neural networks becomes clear when we compare the total costs, especially with dimensions typical for these models. In most neural networks, the hidden layers are much wider than the output layer ($d_2 \gg d_3$).</p> <p>Let’s consider a simple classification problem with a 500-neuron hidden layer ($d_2 = 500$) and 10 output classes ($d_3 = 10$).</p> <ul> <li> <p><strong>Reverse Mode Cost</strong> = $10^2 + (500 \times 10) + 500^2 = 100 + 5,000 + 250,000 = \boldsymbol{255,100}$ operations.</p> </li> <li> <p><strong>Forward Mode Cost</strong> = $(500^2 \times 10) + (500 \times 10^2) + (500 \times 10) = 2,500,000 + 50,000 + 5,000 = \boldsymbol{2,555,000}$ operations.</p> </li> </ul> <p>In this realistic scenario, <strong>forward mode is 10 times more computationally expensive than reverse mode</strong>.</p> <p>This dramatic difference stems from the fact that forward mode requires performing computationally heavy matrix-matrix multiplications, creating large intermediate matrices. Reverse mode (backpropagation) cleverly avoids this by only ever performing matrix-vector multiplications, which is significantly cheaper.</p> <h2 id="the-vjp-trick-never-materialize-jacobians">The VJP Trick: Never Materialize Jacobians</h2> <p>While we’ve established that backprop is the efficient approach for computing gradients, our algorithm can be made even faster and more memory-efficient by avoiding an expensive computational step: materializing Jacobian matrices.</p> <p>We will show a very powerful trick, we <em>never</em> need to materialize any jacobian to pass on the gradients to the inputs.</p> <h3 id="deriving-the-gradient-for-a-linear-layer">Deriving the Gradient for a Linear Layer</h3> <p>Let’s demonstrate this principle using the most fundamental operation in neural networks: matrix multiplication. Consider a linear transformation defined as:</p> \[\begin{align*} y &amp;= Wx \\ \nabla_{W} \mathcal{L} &amp;= J_y^T \nabla_{y} \mathcal{L} \quad \text{From Definition 1 above} \end{align*}\] <p>Let’s expand the above equation fully to observe that the jacobian is <em>sparse</em> (mostly zeros) and simplify the operation further.</p> <p>Let’s define the dimensions: $x \in \mathbb{R}^{d_1 \times 1}$, $W \in \mathbb{R}^{d_2 \times d_1}$, and $y \in \mathbb{R}^{d_2 \times 1}$. The components of the output are $y_i = \sum_{j=1}^{d_1} W_{ij} x_j$.</p> <p>The Jacobian of $y$ with respect to the flattened weight matrix is a $(d_2, d_2 d_1)$ matrix. The derivative $\frac{\partial y_k}{\partial W_{ij}}$ is non-zero only when $k=i$, resulting in a very sparse Jacobian:</p> \[J_y = \begin{pmatrix} \underbrace{x_1 \ \dots \ x_{d_1}}_{W_{1j}} &amp; \underbrace{0 \ \dots \ 0}_{W_{2j}} &amp; \cdots &amp; \underbrace{0 \ \dots \ 0}_{W_{d_2,j}} \\ 0 \ \dots \ 0 &amp; x_1 \ \dots \ x_{d_1} &amp; \cdots &amp; 0 \ \dots \ 0 \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ 0 \ \dots \ 0 &amp; 0 \ \dots \ 0 &amp; \cdots &amp; x_1 \ \dots \ x_{d_1} \end{pmatrix}\] <p>The transposed Jacobian $J_y^T$ is a $(d_2 d_1, d_2)$ matrix. We multiply this by the gradient vector $\nabla_y \mathcal{L} \in \mathbb{R}^{d_2 \times 1}$:</p> \[\nabla_W \mathcal{L} = J_y^T \nabla_y \mathcal{L} = \begin{pmatrix} x_1 &amp; 0 &amp; \cdots &amp; 0 \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ x_{d_1} &amp; 0 &amp; \cdots &amp; 0 \\[0.4em] \hline 0 &amp; x_1 &amp; \cdots &amp; 0 \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ 0 &amp; x_{d_1} &amp; \cdots &amp; 0 \\[0.4em] \hline \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\[0.4em] \hline 0 &amp; 0 &amp; \cdots &amp; x_1 \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ 0 &amp; 0 &amp; \cdots &amp; x_{d_1} \end{pmatrix} \begin{pmatrix} \frac{\partial \mathcal{L}}{\partial y_1} \\ \frac{\partial \mathcal{L}}{\partial y_2} \\ \vdots \\ \frac{\partial \mathcal{L}}{\partial y_{d_2}} \end{pmatrix} = \begin{pmatrix} x_1 \frac{\partial \mathcal{L}}{\partial y_1} \\ \vdots \\ x_{d_1} \frac{\partial \mathcal{L}}{\partial y_1} \\[0.4em] \hline x_1 \frac{\partial \mathcal{L}}{\partial y_2} \\ \vdots \\ x_{d_1} \frac{\partial \mathcal{L}}{\partial y_2} \\[0.4em] \hline \vdots \\[0.4em] \hline x_1 \frac{\partial \mathcal{L}}{\partial y_{d_2}} \\ \vdots \\ x_{d_1} \frac{\partial \mathcal{L}}{\partial y_{d_2}} \end{pmatrix}\] <p>Reshaping this flattened $(d_2 d_1, 1)$ vector back to the $(d_2, d_1)$ dimensions of $W$, we get:</p> \[\nabla_W \mathcal{L} = \begin{pmatrix} \frac{\partial \mathcal{L}}{\partial y_1}x_1 &amp; \frac{\partial \mathcal{L}}{\partial y_1}x_2 &amp; \dots &amp; \frac{\partial \mathcal{L}}{\partial y_1}x_{d_1} \\ \frac{\partial \mathcal{L}}{\partial y_2}x_1 &amp; \frac{\partial \mathcal{L}}{\partial y_2}x_2 &amp; \dots &amp; \frac{\partial \mathcal{L}}{\partial y_2}x_{d_1} \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ \frac{\partial \mathcal{L}}{\partial y_{d_2}}x_1 &amp; \frac{\partial \mathcal{L}}{\partial y_{d_2}}x_2 &amp; \dots &amp; \frac{\partial \mathcal{L}}{\partial y_{d_2}}x_{d_1} \end{pmatrix}\] <p>This matrix is exactly the outer product of the column vector $\nabla_y \mathcal{L}$ and the row vector $x^T$:</p> \[\nabla_y \mathcal{L} \cdot x^T = \begin{pmatrix} \frac{\partial \mathcal{L}}{\partial y_1} \\ \frac{\partial \mathcal{L}}{\partial y_2} \\ \vdots \\ \frac{\partial \mathcal{L}}{\partial y_{d_2}} \end{pmatrix} \begin{pmatrix} x_1 &amp; x_2 &amp; \dots &amp; x_{d_1} \end{pmatrix} = \begin{pmatrix} \frac{\partial \mathcal{L}}{\partial y_1}x_1 &amp; \frac{\partial \mathcal{L}}{\partial y_1}x_2 &amp; \dots &amp; \frac{\partial \mathcal{L}}{\partial y_1}x_{d_1} \\ \frac{\partial \mathcal{L}}{\partial y_2}x_1 &amp; \frac{\partial \mathcal{L}}{\partial y_2}x_2 &amp; \dots &amp; \frac{\partial \mathcal{L}}{\partial y_2}x_{d_1} \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ \frac{\partial \mathcal{L}}{\partial y_{d_2}}x_1 &amp; \frac{\partial \mathcal{L}}{\partial y_{d_2}}x_2 &amp; \dots &amp; \frac{\partial \mathcal{L}}{\partial y_{d_2}}x_{d_1} \end{pmatrix}\] <p>Thus, the Vector-jacobian product (VJP) simplifies to the efficient outer product</p> \[\nabla_W \mathcal{L} = \boxed{ \nabla_y \mathcal{L} \cdot x^T }\] <p>I want you to take a moment to see why this is so efficient. $x$ is the input to our local operation which we already have and $\nabla_{y} \mathcal{L}$ is the gradient of the loss with respect to the output of our local operation and it’s being passed down through the computational graph. We successfully computed the final gradient without ever needing to materialize the full jacobian. This idea can be extended to any operation as long as you define how to pass the gradients from the outputs to the inputs which is exactly what we are going to show in code next using JAX.</p> <h3 id="implementing-custom-vjps-in-jax">Implementing Custom VJPs in JAX</h3> <p>JAX already has a built-in autodiff library that can compute the gradients of any function using <code class="language-plaintext highlighter-rouge">jax.numpy</code> primitives, but we are going to override that using a mechanism in JAX that allows you to write custom gradients for any function you write and then we are going to construct all operations needed to train the simple two layer classifier we have been working with all this time to train on MNIST dataset.</p> <p>Let’s start by defining the simplest operation of matrix multiplication, we will call it <code class="language-plaintext highlighter-rouge">Linear</code></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">jax</span>
<span class="kn">import</span> <span class="n">jax.numpy</span> <span class="k">as</span> <span class="n">jnp</span>

<span class="nd">@jax.custom_vjp</span>
<span class="k">def</span> <span class="nf">Linear</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">W</span> <span class="o">@</span> <span class="n">x</span>

<span class="k">def</span> <span class="nf">Linear_fwd</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="nc">Linear</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">x</span><span class="p">),</span> <span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">Linear_bwd</span><span class="p">(</span><span class="n">res</span><span class="p">,</span> <span class="n">g</span><span class="p">):</span>
    <span class="n">W</span><span class="p">,</span> <span class="n">x</span> <span class="o">=</span> <span class="n">res</span>
    <span class="n">grad_W</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">outer</span><span class="p">(</span><span class="n">g</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
    <span class="n">grad_x</span> <span class="o">=</span> <span class="n">W</span><span class="p">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">g</span>
    <span class="nf">return </span><span class="p">(</span><span class="n">grad_W</span><span class="p">,</span> <span class="n">grad_x</span><span class="p">)</span>

<span class="n">Linear</span><span class="p">.</span><span class="nf">defvjp</span><span class="p">(</span><span class="n">Linear_fwd</span><span class="p">,</span> <span class="n">Linear_bwd</span><span class="p">)</span>
</code></pre></div></div> <p>The <code class="language-plaintext highlighter-rouge">g</code> parameter in <code class="language-plaintext highlighter-rouge">Linear_bwd</code> represents the upstream gradient, that is, the gradient of the final loss with respect to the output of the Linear function.</p> <p>There are two things we need to understand here: first is that we need to define two functions and not just one for the backward pass <code class="language-plaintext highlighter-rouge">bwd</code>, the reason is we need to save some variables that we need during the backward pass, they are saved in a variable called residuals (<code class="language-plaintext highlighter-rouge">res</code>), second is that we need to compute the gradients for both $W$ and $x$ because even though we will only update $W$, we need the gradient to keep flowing through $x$ to the rest of the computational graph.</p> <p>Now let’s define the rest of the operations we need.</p> <p>To further solidify the VJP (Vector-Jacobian Product) concept, let’s consider an element-wise activation function like $a = \tanh(z)$. The derivative of $\tanh(z)$ is $1 - \tanh^2(z)$, and since the operation is applied element-wise, the Jacobian $J_a$ is a <strong>diagonal matrix</strong> with each diagonal entry being the derivative of the corresponding component:</p> \[J_a = \begin{pmatrix} 1 - a_1^2 &amp; 0 &amp; \cdots &amp; 0 \\ 0 &amp; 1 - a_2^2 &amp; \cdots &amp; 0 \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ 0 &amp; 0 &amp; \cdots &amp; 1 - a_{d_2}^2 \end{pmatrix}\] <p>Here, $a_i = \tanh(z_i)$, so each entry is $1 - \tanh^2(z_i) = 1 - a_i^2$.</p> <p>Since the Jacobian is diagonal and the operation is element-wise, the VJP (Vector-Jacobian Product) simplifies to a Hadamard (element-wise) product:</p> \[\nabla_z \mathcal{L} = J_a^T \nabla_a \mathcal{L} = J_a \nabla_a \mathcal{L} = (1 - a^2) \odot \nabla_a \mathcal{L}\] <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 2. Tanh Activation
</span><span class="nd">@jax.custom_vjp</span>
<span class="k">def</span> <span class="nf">Tanh</span><span class="p">(</span><span class="n">z</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">tanh</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">Tanh_fwd</span><span class="p">(</span><span class="n">z</span><span class="p">):</span>
    <span class="n">a</span> <span class="o">=</span> <span class="nc">Tanh</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">a</span><span class="p">,</span> <span class="p">(</span><span class="n">a</span><span class="p">,)</span>

<span class="k">def</span> <span class="nf">Tanh_bwd</span><span class="p">(</span><span class="n">res</span><span class="p">,</span> <span class="n">g</span><span class="p">):</span>
    <span class="n">a</span><span class="p">,</span> <span class="o">=</span> <span class="n">res</span>
    <span class="n">grad_z</span> <span class="o">=</span> <span class="n">g</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">a</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
    <span class="nf">return </span><span class="p">(</span><span class="n">grad_z</span><span class="p">,)</span> <span class="c1"># Gradient for the single input z
</span>
<span class="n">Tanh</span><span class="p">.</span><span class="nf">defvjp</span><span class="p">(</span><span class="n">Tanh_fwd</span><span class="p">,</span> <span class="n">Tanh_bwd</span><span class="p">)</span>
</code></pre></div></div> <h3 id="deriving-the-softmax-cross-entropy-gradient">Deriving the Softmax Cross-Entropy Gradient</h3> <p>The tanh is quite straightforward and you could easily verify it by hand. One important thing to realize is there is no restriction on the complexity of the operation as long as we <em>know</em> how to flow the gradients from the outputs to the inputs. So we have two options now either first define a softmax operation and then define another operation for the cross-entropy loss or combine them in a single operation. There are multiple reasons why you would want to combine operations into a single operation (e.g., numerical stability) but we will do because the gradients will be very simple. Let’s derive the gradients for the combined Softmax + Cross entropy operation and then implement <code class="language-plaintext highlighter-rouge">softmax_cross_entropy</code> function.</p> <p><strong>Note:</strong> here $y$ and $z$ refer to the logits and true labels respectively, not to be confused with the $y$ and $z$ used in the two-layer network example above.</p> <p>First, let’s define the core functions. We have a set of logits (the raw output of the last linear layer), $z = [z_1, z_2, …, z_C]$, where $C$ is the number of classes. The true label is a one-hot encoded vector $y = [y_1, y_2, …, y_C]$, where only one $y_k=1$ and all others are 0.</p> <p><strong>Softmax Function:</strong> Converts logits into probabilities. The probability for the $j$-th class, $p_j$, is:</p> \[p_j = \text{softmax}(z)_j = \frac{e^{z_j}}{\sum_{k=1}^{C} e^{z_k}}\] <p><strong>Cross-Entropy Loss:</strong> Calculates the loss based on the predicted probabilities $p$ and the true labels $y$.</p> \[\mathcal{L} = - \sum_{k=1}^{C} y_k \log(p_k)\] <p>The loss $\mathcal{L}$ is not a direct function of $z_i$. Instead, $\mathcal{L}$ depends on all the probabilities $p_1, p_2, …, p_C$, and each of these probabilities in turn depends on the logit $z_i$. Therefore, we must use the chain rule and sum over all paths through which $z_i$ affects $\mathcal{L}$:</p> \[\frac{\partial \mathcal{L} }{\partial z_i} = \sum_{j=1}^{C} \frac{\partial \mathcal{L} }{\partial p_j} \frac{\partial p_j}{\partial z_i}\] <p>We will solve this by calculating the two partial derivatives separately.</p> <p><strong>Derivative of Loss with respect to Probabilities</strong></p> <p>This part is straightforward. We differentiate the loss function $\mathcal{L} = - \sum_{k=1}^{C} y_k \log(p_k)$ with respect to a single probability $p_j$. Only the term where $k=j$ is non-zero.</p> \[\frac{\partial \mathcal{L} }{\partial p_j} = \frac{\partial}{\partial p_j} \left( - y_j \log(p_j) \right) = - \frac{y_j}{p_j}\] <p><strong>Derivative of Softmax with respect to Logits</strong></p> <p>This part is more complex because the output of softmax at index $j$ depends on all the input logits. We have two cases for $\frac{\partial p_j}{\partial z_i}$.</p> <p><strong>Case A: When $i = j$</strong> (e.g., differentiating $p_i$ with respect to $z_i$) We use the quotient rule on $p_i = \frac{e^{z_i}}{\sum_{k} e^{z_k}}$:</p> \[\begin{align*} \frac{\partial p_i}{\partial z_i} &amp;= \frac{(e^{z_i})' (\sum_{k} e^{z_k}) - (e^{z_i}) (\sum_{k} e^{z_k})'}{(\sum_{k} e^{z_k})^2}\\ &amp;= \frac{e^{z_i} (\sum_{k} e^{z_k}) - e^{z_i} (e^{z_i})}{(\sum_{k} e^{z_k})^2}\\ &amp;= \frac{e^{z_i}}{\sum_{k} e^{z_k}} - \left(\frac{e^{z_i}}{\sum_{k} e^{z_k}}\right)^2 = p_i - p_i^2 = p_i(1 - p_i) \end{align*}\] <p><strong>Case B: When $i \neq j$</strong> (e.g., differentiating $p_j$ with respect to $z_i$) Again, using the quotient rule on $p_j = \frac{e^{z_j}}{\sum_{k} e^{z_k}}$. This time, the numerator $e^{z_j}$ is a constant with respect to $z_i$.</p> \[\begin{align*} \frac{\partial p_j}{\partial z_i} &amp;= \frac{(e^{z_j})' (\sum_{k} e^{z_k}) - (e^{z_j}) (\sum_{k} e^{z_k})'}{(\sum_{k} e^{z_k})^2}\\ &amp;= \frac{0 \cdot (\sum_{k} e^{z_k}) - e^{z_j} (e^{z_i})}{(\sum_{k} e^{z_k})^2}\\ &amp;= - \left(\frac{e^{z_j}}{\sum_{k} e^{z_k}}\right) \left(\frac{e^{z_i}}{\sum_{k} e^{z_k}}\right) = - p_j p_i \end{align*}\] <p>Now we substitute these results back into our chain rule sum. We split the sum into two parts: the term where $j=i$ and all the terms where $j \neq i$.</p> \[\frac{\partial \mathcal{L} }{\partial z_i} = \underbrace{\frac{\partial \mathcal{L} }{\partial p_i} \frac{\partial p_i}{\partial z_i}}_{\text{Term for } j=i} + \underbrace{\sum_{j \neq i} \frac{\partial \mathcal{L} }{\partial p_j} \frac{\partial p_j}{\partial z_i}}_{\text{Terms for } j \neq i}\] <p>Substitute our findings from the previous steps and simplify</p> \[\begin{align*} \frac{\partial \mathcal{L} }{\partial z_i} &amp;= \left(-\frac{y_i}{p_i}\right) \cdot \left(p_i(1-p_i)\right) + \sum_{j \neq i} \left(-\frac{y_j}{p_j}\right) \cdot \left(-p_j p_i\right)\\ &amp;= -y_i(1-p_i) + \sum_{j \neq i} y_j p_i \\ &amp;= -y_i + y_i p_i + p_i \sum_{j \neq i} y_j \\ &amp;= -y_i + p_i \left( y_i + \sum_{j \neq i} y_j \right) \\ &amp;= -y_i + p_i(1) \\ &amp;= \boxed{p_i - y_i} \end{align*}\] <p>This remarkable result shows that the gradient of the combined softmax and cross-entropy loss with respect to a single logit $z_i$ is simply the difference between the predicted probability for that class and the true label for that class.</p> <p>When we generalize this from a single logit to the entire vector of logits $z$, the gradient vector is:</p> \[\nabla_z \mathcal{L} = p - y\] <p>This is precisely what we implemented in the following <code class="language-plaintext highlighter-rouge">softmax_cross_entropy_bwd</code> function: <code class="language-plaintext highlighter-rouge">probs - labels_res</code>.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nd">@jax.custom_vjp</span>
<span class="k">def</span> <span class="nf">softmax_cross_entropy</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">label</span><span class="p">):</span>
    <span class="c1"># This is the forward definition of the function
</span>    <span class="n">max_logit</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">max</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span>
    <span class="n">exps</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">logits</span> <span class="o">-</span> <span class="n">max_logit</span><span class="p">)</span>
    <span class="n">probs</span> <span class="o">=</span> <span class="n">exps</span> <span class="o">/</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">exps</span><span class="p">)</span>
    <span class="k">return</span> <span class="o">-</span><span class="n">jnp</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">label</span> <span class="o">*</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="n">probs</span> <span class="o">+</span> <span class="mf">1e-7</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">softmax_cross_entropy_fwd</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">label</span><span class="p">):</span>
    <span class="c1"># Forward pass for VJP: compute output and save residuals
</span>    <span class="n">loss</span> <span class="o">=</span> <span class="nf">softmax_cross_entropy</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span>
    <span class="c1"># Re-compute probabilities here to save them as residuals for the backward pass.
</span>    <span class="n">max_logit</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">max</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span>
    <span class="n">exps</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">logits</span> <span class="o">-</span> <span class="n">max_logit</span><span class="p">)</span>
    <span class="n">probs</span> <span class="o">=</span> <span class="n">exps</span> <span class="o">/</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">exps</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">loss</span><span class="p">,</span> <span class="p">(</span><span class="n">probs</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">softmax_cross_entropy_bwd</span><span class="p">(</span><span class="n">res</span><span class="p">,</span> <span class="n">g</span><span class="p">):</span>
    <span class="c1"># Backward pass for VJP
</span>    <span class="n">probs</span><span class="p">,</span> <span class="n">label_res</span> <span class="o">=</span> <span class="n">res</span>
    <span class="c1"># Gradient with respect to logits
</span>    <span class="n">grad_logits</span> <span class="o">=</span> <span class="n">g</span> <span class="o">*</span> <span class="p">(</span><span class="n">probs</span> <span class="o">-</span> <span class="n">label_res</span><span class="p">)</span>
    <span class="c1"># The gradient for 'label' is not needed, so we return None.
</span>    <span class="c1"># The returned tuple must match the number of inputs to the function.
</span>    <span class="nf">return </span><span class="p">(</span><span class="n">grad_logits</span><span class="p">,</span> <span class="bp">None</span><span class="p">)</span>

<span class="n">softmax_cross_entropy</span><span class="p">.</span><span class="nf">defvjp</span><span class="p">(</span><span class="n">softmax_cross_entropy_fwd</span><span class="p">,</span> <span class="n">softmax_cross_entropy_bwd</span><span class="p">)</span>
</code></pre></div></div> <h2 id="putting-it-all-together-a-full-example">Putting It All Together: A Full Example</h2> <p>Now, I will combine all the pieces together to define the two-layer neural network and train it on the MNIST dataset using JAX with our custom backpropagation implementations in a single script of 150 lines.</p> <h3 id="training-a-neural-network-on-mnist">Training a Neural Network on MNIST</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">jax</span>
<span class="kn">import</span> <span class="n">jax.numpy</span> <span class="k">as</span> <span class="n">jnp</span>
<span class="kn">from</span> <span class="n">jax</span> <span class="kn">import</span> <span class="n">grad</span><span class="p">,</span> <span class="n">jit</span><span class="p">,</span> <span class="n">value_and_grad</span><span class="p">,</span> <span class="n">vmap</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="kn">import</span> <span class="n">torch</span>
<span class="kn">from</span> <span class="n">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>
<span class="kn">from</span> <span class="n">torchvision.datasets</span> <span class="kn">import</span> <span class="n">MNIST</span>
<span class="kn">from</span> <span class="n">torchvision</span> <span class="kn">import</span> <span class="n">transforms</span>


<span class="c1"># 1. Linear Layer
</span><span class="nd">@jax.custom_vjp</span>
<span class="k">def</span> <span class="nf">Linear</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">W</span> <span class="o">@</span> <span class="n">x</span>

<span class="k">def</span> <span class="nf">Linear_fwd</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="nc">Linear</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">x</span><span class="p">),</span> <span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">Linear_bwd</span><span class="p">(</span><span class="n">res</span><span class="p">,</span> <span class="n">g</span><span class="p">):</span>
    <span class="n">W</span><span class="p">,</span> <span class="n">x</span> <span class="o">=</span> <span class="n">res</span>
    <span class="n">grad_W</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">outer</span><span class="p">(</span><span class="n">g</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
    <span class="n">grad_x</span> <span class="o">=</span> <span class="n">W</span><span class="p">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">g</span>
    <span class="nf">return </span><span class="p">(</span><span class="n">grad_W</span><span class="p">,</span> <span class="n">grad_x</span><span class="p">)</span>

<span class="n">Linear</span><span class="p">.</span><span class="nf">defvjp</span><span class="p">(</span><span class="n">Linear_fwd</span><span class="p">,</span> <span class="n">Linear_bwd</span><span class="p">)</span>

<span class="nd">@jax.custom_vjp</span>
<span class="k">def</span> <span class="nf">Tanh</span><span class="p">(</span><span class="n">z</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">tanh</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">Tanh_fwd</span><span class="p">(</span><span class="n">z</span><span class="p">):</span>
    <span class="n">a</span> <span class="o">=</span> <span class="nc">Tanh</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">a</span><span class="p">,</span> <span class="p">(</span><span class="n">a</span><span class="p">,)</span>

<span class="k">def</span> <span class="nf">Tanh_bwd</span><span class="p">(</span><span class="n">res</span><span class="p">,</span> <span class="n">g</span><span class="p">):</span>
    <span class="n">a</span><span class="p">,</span> <span class="o">=</span> <span class="n">res</span>
    <span class="n">grad_z</span> <span class="o">=</span> <span class="n">g</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">a</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
    <span class="nf">return </span><span class="p">(</span><span class="n">grad_z</span><span class="p">,)</span>

<span class="n">Tanh</span><span class="p">.</span><span class="nf">defvjp</span><span class="p">(</span><span class="n">Tanh_fwd</span><span class="p">,</span> <span class="n">Tanh_bwd</span><span class="p">)</span>

<span class="nd">@jax.custom_vjp</span>
<span class="k">def</span> <span class="nf">softmax_cross_entropy</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">label</span><span class="p">):</span>
    <span class="n">max_logit</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">max</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span>
    <span class="n">exps</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">logits</span> <span class="o">-</span> <span class="n">max_logit</span><span class="p">)</span>
    <span class="n">probs</span> <span class="o">=</span> <span class="n">exps</span> <span class="o">/</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">exps</span><span class="p">)</span>
    <span class="k">return</span> <span class="o">-</span><span class="n">jnp</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">label</span> <span class="o">*</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="n">probs</span> <span class="o">+</span> <span class="mf">1e-7</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">softmax_cross_entropy_fwd</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">label</span><span class="p">):</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="nf">softmax_cross_entropy</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span>
    <span class="c1"># Re-compute probabilities here to save them as residuals for the backward pass.
</span>    <span class="n">max_logit</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">max</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span>
    <span class="n">exps</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">logits</span> <span class="o">-</span> <span class="n">max_logit</span><span class="p">)</span>
    <span class="n">probs</span> <span class="o">=</span> <span class="n">exps</span> <span class="o">/</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">exps</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">loss</span><span class="p">,</span> <span class="p">(</span><span class="n">probs</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">softmax_cross_entropy_bwd</span><span class="p">(</span><span class="n">res</span><span class="p">,</span> <span class="n">g</span><span class="p">):</span>
    <span class="n">probs</span><span class="p">,</span> <span class="n">label_res</span> <span class="o">=</span> <span class="n">res</span>
    <span class="n">grad_logits</span> <span class="o">=</span> <span class="n">g</span> <span class="o">*</span> <span class="p">(</span><span class="n">probs</span> <span class="o">-</span> <span class="n">label_res</span><span class="p">)</span>
    <span class="c1"># The gradient for 'label' is not needed, so we return None.
</span>    <span class="c1"># The returned tuple must match the number of inputs to the function.
</span>    <span class="nf">return </span><span class="p">(</span><span class="n">grad_logits</span><span class="p">,</span> <span class="bp">None</span><span class="p">)</span>

<span class="n">softmax_cross_entropy</span><span class="p">.</span><span class="nf">defvjp</span><span class="p">(</span><span class="n">softmax_cross_entropy_fwd</span><span class="p">,</span> <span class="n">softmax_cross_entropy_bwd</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">init_params</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">sizes</span><span class="p">):</span>
    <span class="n">keys</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="nf">len</span><span class="p">(</span><span class="n">sizes</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">jax</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">normal</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="p">(</span><span class="n">out_size</span><span class="p">,</span> <span class="n">in_size</span><span class="p">))</span> <span class="o">*</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="mf">2.</span> <span class="o">/</span> <span class="n">in_size</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">in_size</span><span class="p">,</span> <span class="n">out_size</span> <span class="ow">in</span> <span class="nf">zip</span><span class="p">(</span><span class="n">keys</span><span class="p">,</span> <span class="n">sizes</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">sizes</span><span class="p">[</span><span class="mi">1</span><span class="p">:])]</span>

<span class="k">def</span> <span class="nf">forward_pass</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">image</span><span class="p">):</span>
    <span class="n">W1</span><span class="p">,</span> <span class="n">W2</span> <span class="o">=</span> <span class="n">params</span>
    <span class="n">z1</span> <span class="o">=</span> <span class="nc">Linear</span><span class="p">(</span><span class="n">W1</span><span class="p">,</span> <span class="n">image</span><span class="p">)</span>
    <span class="n">a1</span> <span class="o">=</span> <span class="nc">Tanh</span><span class="p">(</span><span class="n">z1</span><span class="p">)</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="nc">Linear</span><span class="p">(</span><span class="n">W2</span><span class="p">,</span> <span class="n">a1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">logits</span>

<span class="c1"># Vectorize the forward pass to handle batches efficiently
</span><span class="n">batch_forward</span> <span class="o">=</span> <span class="nf">vmap</span><span class="p">(</span><span class="n">forward_pass</span><span class="p">,</span> <span class="n">in_axes</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">batch_loss_fn</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">images</span><span class="p">,</span> <span class="n">labels</span><span class="p">):</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="nf">batch_forward</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">images</span><span class="p">)</span>
    <span class="c1"># Vmap the single-example loss function over the batch
</span>    <span class="n">losses</span> <span class="o">=</span> <span class="nf">vmap</span><span class="p">(</span><span class="n">softmax_cross_entropy</span><span class="p">)(</span><span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">losses</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">accuracy</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">images</span><span class="p">,</span> <span class="n">labels</span><span class="p">):</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="nf">batch_forward</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">images</span><span class="p">)</span>
    <span class="n">predicted_class</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">true_class</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">predicted_class</span> <span class="o">==</span> <span class="n">true_class</span><span class="p">)</span>

<span class="nd">@jit</span>
<span class="k">def</span> <span class="nf">update_step</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">images</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">):</span>
    <span class="n">loss</span><span class="p">,</span> <span class="n">grads</span> <span class="o">=</span> <span class="nf">value_and_grad</span><span class="p">(</span><span class="n">batch_loss_fn</span><span class="p">)(</span><span class="n">params</span><span class="p">,</span> <span class="n">images</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
    <span class="n">updated_params</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">tree_util</span><span class="p">.</span><span class="nf">tree_map</span><span class="p">(</span>
        <span class="k">lambda</span> <span class="n">p</span><span class="p">,</span> <span class="n">g</span><span class="p">:</span> <span class="n">p</span> <span class="o">-</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">g</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">grads</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">updated_params</span><span class="p">,</span> <span class="n">loss</span>

<span class="c1"># --- PyTorch Data Loading and Training Loop ---
</span>
<span class="k">def</span> <span class="nf">load_mnist_data_pytorch</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">128</span><span class="p">):</span>
    <span class="n">transform</span> <span class="o">=</span> <span class="n">transforms</span><span class="p">.</span><span class="nc">Compose</span><span class="p">([</span>
        <span class="n">transforms</span><span class="p">.</span><span class="nc">ToTensor</span><span class="p">(),</span>
        <span class="n">transforms</span><span class="p">.</span><span class="nc">Normalize</span><span class="p">((</span><span class="mf">0.1307</span><span class="p">,),</span> <span class="p">(</span><span class="mf">0.3081</span><span class="p">,))</span>
    <span class="p">])</span>
    <span class="n">train_dataset</span> <span class="o">=</span> <span class="nc">MNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="sh">'</span><span class="s">./data</span><span class="sh">'</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">)</span>
    <span class="n">test_dataset</span> <span class="o">=</span> <span class="nc">MNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="sh">'</span><span class="s">./data</span><span class="sh">'</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">)</span>

    <span class="n">train_loader</span> <span class="o">=</span> <span class="nc">DataLoader</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">test_loader</span> <span class="o">=</span> <span class="nc">DataLoader</span><span class="p">(</span><span class="n">test_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">train_loader</span><span class="p">,</span> <span class="n">test_loader</span>

<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="sh">'</span><span class="s">__main__</span><span class="sh">'</span><span class="p">:</span>
    <span class="n">train_loader</span><span class="p">,</span> <span class="n">test_loader</span> <span class="o">=</span> <span class="nf">load_mnist_data_pytorch</span><span class="p">()</span>

    <span class="c1"># Network architecture: 784 -&gt; 512 -&gt; 10
</span>    <span class="n">layer_sizes</span> <span class="o">=</span> <span class="p">[</span><span class="mi">784</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="mi">10</span><span class="p">]</span>
    <span class="n">key</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nc">PRNGKey</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">params</span> <span class="o">=</span> <span class="nf">init_params</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">layer_sizes</span><span class="p">)</span>

    <span class="n">epochs</span> <span class="o">=</span> <span class="mi">10</span>
    <span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.01</span>

    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Starting training...</span><span class="sh">"</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
        <span class="n">epoch_loss</span> <span class="o">=</span> <span class="mf">0.0</span>
        <span class="n">num_batches</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="ow">in</span> <span class="n">train_loader</span><span class="p">:</span>
            <span class="n">images_np</span> <span class="o">=</span> <span class="n">images</span><span class="p">.</span><span class="nf">numpy</span><span class="p">()</span>
            <span class="n">labels_np</span> <span class="o">=</span> <span class="n">labels</span><span class="p">.</span><span class="nf">numpy</span><span class="p">()</span>

            <span class="n">images_np</span> <span class="o">=</span> <span class="n">images_np</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="n">images_np</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">labels_one_hot</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nf">one_hot</span><span class="p">(</span><span class="n">labels_np</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>

            <span class="n">params</span><span class="p">,</span> <span class="n">loss_val</span> <span class="o">=</span> <span class="nf">update_step</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">images_np</span><span class="p">,</span> <span class="n">labels_one_hot</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">)</span>
            <span class="n">epoch_loss</span> <span class="o">+=</span> <span class="n">loss_val</span>
            <span class="n">num_batches</span> <span class="o">+=</span> <span class="mi">1</span>

        <span class="n">avg_loss</span> <span class="o">=</span> <span class="n">epoch_loss</span> <span class="o">/</span> <span class="n">num_batches</span>

        <span class="n">test_images_tensor</span><span class="p">,</span> <span class="n">test_labels_tensor</span> <span class="o">=</span> <span class="nf">next</span><span class="p">(</span><span class="nf">iter</span><span class="p">(</span><span class="n">test_loader</span><span class="p">))</span>
        <span class="n">test_images_np</span> <span class="o">=</span> <span class="n">test_images_tensor</span><span class="p">.</span><span class="nf">numpy</span><span class="p">().</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">784</span><span class="p">)</span>
        <span class="n">test_labels_one_hot</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nf">one_hot</span><span class="p">(</span><span class="n">test_labels_tensor</span><span class="p">.</span><span class="nf">numpy</span><span class="p">(),</span> <span class="mi">10</span><span class="p">)</span>

        <span class="n">test_acc</span> <span class="o">=</span> <span class="nf">accuracy</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">test_images_np</span><span class="p">,</span> <span class="n">test_labels_one_hot</span><span class="p">)</span>

        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Epoch </span><span class="si">{</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="si">}</span><span class="s">/</span><span class="si">{</span><span class="n">epochs</span><span class="si">}</span><span class="s"> | Loss: </span><span class="si">{</span><span class="n">avg_loss</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="s"> | Test Accuracy: </span><span class="si">{</span><span class="n">test_acc</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <h3 id="results-and-verification">Results and Verification</h3> <p>Let’s see if our definitions worked as expected by running the training script:</p> <div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Starting training...
Epoch 1/10 | Loss: 0.5898 | Test Accuracy: 0.9009
Epoch 2/10 | Loss: 0.3269 | Test Accuracy: 0.9187
Epoch 3/10 | Loss: 0.2807 | Test Accuracy: 0.9262
Epoch 4/10 | Loss: 0.2536 | Test Accuracy: 0.9323
Epoch 5/10 | Loss: 0.2341 | Test Accuracy: 0.9363
Epoch 6/10 | Loss: 0.2186 | Test Accuracy: 0.9387
Epoch 7/10 | Loss: 0.2056 | Test Accuracy: 0.9428
Epoch 8/10 | Loss: 0.1943 | Test Accuracy: 0.9448
Epoch 9/10 | Loss: 0.1844 | Test Accuracy: 0.9466
Epoch 10/10 | Loss: 0.1755 | Test Accuracy: 0.9487
</code></pre></div></div> <p>The model achieves over 94% accuracy on the MNIST test set after just 10 epochs of training, demonstrating that our custom backpropagation implementations are functioning correctly!</p> <p>Let’s visualize some predictions from the trained model to see how well it performs.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/inline/backprop-mnist-480.webp 480w,/assets/img/inline/backprop-mnist-800.webp 800w,/assets/img/inline/backprop-mnist-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/inline/backprop-mnist.png" class="img-fluid rounded d-block mx-auto" width="900px" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">The model's predictions on some MNIST test images.</figcaption> </figure> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/ERC/">Sui Generis (ERC)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/LA-01/">Linear Algebra Part 01: Identities</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/python-heapq/">Reimplementing python's heapq module</a> </li> <div id="giscus_thread" style="max-width: 930px; margin: 0 auto;"> <script>
      let giscusTheme = determineComputedTheme();
      let giscusAttributes = {
        src: 'https://giscus.app/client.js',
        'data-repo': '0xgeorgeassaad/0xgeorgeassaad.github.io',
        'data-repo-id': 'R_kgDOPK-4Vg',
        'data-category': 'Comments',
        'data-category-id': 'DIC_kwDOPK-4Vs4Cs1UA',
        'data-mapping': 'title',
        'data-strict': '1',
        'data-reactions-enabled': '1',
        'data-emit-metadata': '0',
        'data-input-position': 'bottom',
        'data-theme': giscusTheme,
        'data-lang': 'en',
        crossorigin: 'anonymous',
        async: '',
      };

      let giscusScript = document.createElement('script');
      Object.entries(giscusAttributes).forEach(([key, value]) => giscusScript.setAttribute(key, value));
      document.getElementById('giscus_thread').appendChild(giscusScript);
    </script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> © Copyright 2025 George Assaad. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-0D9DTLDYE6"></script> <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() {
      dataLayer.push(arguments);
    }
    gtag('js', new Date());

    gtag('config', 'G-0D9DTLDYE6');
  </script> <script defer src="/assets/js/google-analytics-setup.js"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>